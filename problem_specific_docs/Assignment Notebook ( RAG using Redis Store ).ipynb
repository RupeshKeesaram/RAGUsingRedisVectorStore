{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78f1d91f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:01:00.266176Z",
     "start_time": "2025-01-23T04:01:00.261077Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09d011e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:01:03.718487Z",
     "start_time": "2025-01-23T04:01:01.885038Z"
    }
   },
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.vectorstores import Redis\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bf1ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b28461da",
   "metadata": {},
   "source": [
    "# Document Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfd1f138",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:14:11.109675Z",
     "start_time": "2025-01-23T04:14:09.514624Z"
    }
   },
   "outputs": [],
   "source": [
    "# As of now I'm considering only one document for the RAG, so I'll be going with PyPDFLoader. \n",
    "# I'll try use directory loader in the backend if required\n",
    "\n",
    "# loading a document using PyPDFLoader\n",
    "\n",
    "data_path = r\"C:\\Users\\KeesaramRupesh\\Downloads\\NBEATS paper.pdf\"\n",
    "data_loader = PyPDFLoader(data_path)\n",
    "\n",
    "data = data_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ea318fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:14:11.118708Z",
     "start_time": "2025-01-23T04:14:11.112413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Pages Available :31\n"
     ]
    }
   ],
   "source": [
    "# len of the document\n",
    "\n",
    "print(f\"Total Pages Available :{len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b50e65d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:14:11.150501Z",
     "start_time": "2025-01-23T04:14:11.120726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Published as a conference paper at ICLR 2020\n",
      "N-BEATS: N EURAL BASIS EXPANSION ANALYSIS FOR\n",
      "INTERPRETABLE TIME SERIES FORECASTING\n",
      "Boris N. Oreshkin\n",
      "Element AI\n",
      "boris.oreshkin@gmail.comDmitri Carpov\n",
      "Element AI\n",
      "dmitri.carpov@elementai.com\n",
      "Nicolas Chapados\n",
      "Element AI\n",
      "chapados@elementai.comYoshua Bengio\n",
      "Mila\n",
      "yoshua.bengio@mila.quebec\n",
      "ABSTRACT\n",
      "We focus on solving the univariate times series point forecasting problem using\n",
      "deep learning. We propose a deep neural architecture based on backward and\n",
      "forward residual links and a very deep stack of fully-connected layers. The ar-\n",
      "chitecture has a number of desirable properties, being interpretable, applicable\n",
      "without modiﬁcation to a wide array of target domains, and fast to train. We test\n",
      "the proposed architecture on several well-known datasets, including M3, M4 and\n",
      "TOURISM competition datasets containing time series from diverse domains. We\n",
      "demonstrate state-of-the-art performance for two conﬁgurations of N-BEATS for\n",
      "all the datasets, improving forecast accuracy by 11% over a statistical benchmark\n",
      "and by 3% over last year’s winner of the M4 competition, a domain-adjusted\n",
      "hand-crafted hybrid between neural network and statistical time series models.\n",
      "The ﬁrst conﬁguration of our model does not employ any time-series-speciﬁc\n",
      "components and its performance on heterogeneous datasets strongly suggests that,\n",
      "contrarily to received wisdom, deep learning primitives such as residual blocks are\n",
      "by themselves sufﬁcient to solve a wide range of forecasting problems. Finally, we\n",
      "demonstrate how the proposed architecture can be augmented to provide outputs\n",
      "that are interpretable without considerable loss in accuracy.\n",
      "1 I NTRODUCTION\n",
      "Time series (TS) forecasting is an important business problem and a fruitful application area for\n",
      "machine learning (ML). It underlies most aspects of modern business, including such critical areas as\n",
      "inventory control and customer management, as well as business planning going from production and\n",
      "distribution to ﬁnance and marketing. As such, it has a considerable ﬁnancial impact, often ranging\n",
      "in the millions of dollars for every point of forecasting accuracy gained (Jain, 2017; Kahn, 2003).\n",
      "And yet, unlike areas such as computer vision or natural language processing where deep learning\n",
      "(DL) techniques are now well entrenched, there still exists evidence that ML and DL struggle to\n",
      "outperform classical statistical TS forecasting approaches (Makridakis et al., 2018a;b). For instance,\n",
      "the rankings of the six “pure” ML methods submitted to M4 competition were 23, 37, 38, 48, 54,\n",
      "and 57 out of a total of 60 entries, and most of the best-ranking methods were ensembles of classical\n",
      "statistical techniques (Makridakis et al., 2018b).\n",
      "On the other hand, the M4 competition winner (Smyl, 2020), was based on a hybrid between\n",
      "neural residual/attention dilated LSTM stack with a classical Holt-Winters statistical model (Holt,\n",
      "1957; 2004; Winters, 1960) with learnable parameters. Since Smyl’s approach heavily depends on\n",
      "this Holt-Winters component, Makridakis et al. (2018b) further argue that “hybrid approaches and\n",
      "combinations of method are the way forward for improving the forecasting accuracy and making\n",
      "forecasting more valuable”. In this work we aspire to challenge this conclusion by exploring the\n",
      "potential of pure DL architectures in the context of the TS forecasting. Moreover, in the context of\n",
      "interpretable DL architecture design, we are interested in answering the following question: can we\n",
      "1arXiv:1905.10437v4  [cs.LG]  20 Feb 2020' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 0}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "inject a suitable inductive bias in the model to make its internal operations more interpretable, in the\n",
      "sense of extracting some explainable driving factors combining to produce a given forecast?\n",
      "1.1 S UMMARY OF CONTRIBUTIONS\n",
      "Deep Neural Architecture: To the best of our knowledge, this is the ﬁrst work to empirically\n",
      "demonstrate that pure DL using no time-series speciﬁc components outperforms well-established\n",
      "statistical approaches on M3, M4 and TOURISM datasets (on M4, by 11% over statistical benchmark,\n",
      "by 7% over the best statistical entry, and by 3% over the M4 competition winner). In our view, this\n",
      "provides a long-missing proof of concept for the use of pure ML in TS forecasting and strengthens\n",
      "motivation to continue advancing the research in this area.\n",
      "Interpretable DL for Time Series: In addition to accuracy beneﬁts, we also show that it is fea-\n",
      "sible to design an architecture with interpretable outputs that can be used by practitioners in very\n",
      "much the same way as traditional decomposition techniques such as the “seasonality-trend-level”\n",
      "approach (Cleveland et al., 1990).\n",
      "2 P ROBLEM STATEMENT\n",
      "We consider the univariate point forecasting problem in discrete time. Given a length- Hforecast\n",
      "horizon a length- Tobserved series history [y1,..., yT]∈RT, the task is to predict the vector of\n",
      "future values y∈RH= [yT+1,yT+2,..., yT+H]. For simplicity, we will later consider a lookback\n",
      "window of length t≤Tending with the last observed value yTto serve as model input, and denoted\n",
      "x∈Rt= [yT−t+1,..., yT]. We denoteˆythe forecast of y. The following metrics are commonly\n",
      "used to evaluate forecasting performance (Hyndman & Koehler, 2006; Makridakis & Hibon, 2000;\n",
      "Makridakis et al., 2018b; Athanasopoulos et al., 2011):\n",
      "sMAPE =200\n",
      "HH\n",
      "∑\n",
      "i=1|yT+i−ˆyT+i|\n",
      "|yT+i|+|ˆyT+i|, MAPE =100\n",
      "HH\n",
      "∑\n",
      "i=1|yT+i−ˆyT+i|\n",
      "|yT+i|,\n",
      "MASE =1\n",
      "HH\n",
      "∑\n",
      "i=1|yT+i−ˆyT+i|\n",
      "1\n",
      "T+H−m∑T+H\n",
      "j=m+1|yj−yj−m|, OWA=1\n",
      "2[sMAPE\n",
      "sMAPE Naïve2+MASE\n",
      "MASE Naïve2]\n",
      ".\n",
      "Here mis the periodicity of the data ( e.g., 12 for monthly series). MAPE (Mean Absolute Percentage\n",
      "Error), sMAPE (symmetric MAPE ) and MASE (Mean Absolute Scaled Error) are standard scale-free\n",
      "metrics in the practice of forecasting (Hyndman & Koehler, 2006; Makridakis & Hibon, 2000):\n",
      "whereas sMAPE scales the error by the average between the forecast and ground truth, the MASE\n",
      "scales by the average error of the naïve predictor that simply copies the observation measured m\n",
      "periods in the past, thereby accounting for seasonality. OWA (overall weighted average) is a M4-\n",
      "speciﬁc metric used to rank competition entries (M4 Team, 2018b), where sMAPE and MASE metrics\n",
      "are normalized such that a seasonally-adjusted naïve forecast obtains OWA=1.0.\n",
      "3 N-BEATS\n",
      "Our architecture design methodology relies on a few key principles. First, the base architecture\n",
      "should be simple and generic, yet expressive (deep). Second, the architecture should not rely on time-\n",
      "series-speciﬁc feature engineering or input scaling. These prerequisites let us explore the potential\n",
      "of pure DL architecture in TS forecasting. Finally, as a prerequisite to explore interpretability, the\n",
      "architecture should be extendable towards making its outputs human interpretable. We now discuss\n",
      "how those principles converge to the proposed architecture.\n",
      "3.1 B ASIC BLOCK\n",
      "The proposed basic building block has a fork architecture and is depicted in Fig. 1 (left). We focus on\n",
      "describing the operation of ℓ-th block in this section in detail (note that the block index ℓis dropped\n",
      "in Fig. 1 for brevity). The ℓ-th block accepts its respective input xℓand outputs two vectors, ˆxℓand\n",
      "ˆyℓ. For the very ﬁrst block in the model, its respective xℓis the overall model input — a history\n",
      "lookback window of certain length ending with the last measured observation. We set the length of\n",
      "2' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 1}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "FC Stack\n",
      "(4 layers)\n",
      "FC FC\n",
      "( ) \u0000\u0000\u0000\u0000( ) \u0000\u0000\u0000\u0000\n",
      "Forecast Backcast\u0000\u0000Block Input\n",
      "\u0000\u0000Block 1\n",
      "Block 2\n",
      "Block  K–\n",
      "–\n",
      "–+\n",
      "Stack residual\n",
      "(to next stack)Stack Input\n",
      "Stack\n",
      "forecastStack 1\n",
      "Stack 2\n",
      "Stack M+Global forecast\n",
      "(model output)Lookback window\n",
      "(model input)Forecast Period\n",
      "Horizon HLookback Period\n",
      "Horizon nH (here n=3)\n",
      "Figure 1: Proposed architecture. The basic building block is a multi-layer FC network with RELU\n",
      "nonlinearities. It predicts basis expansion coefﬁcients both forward, θf, (forecast) and backward, θb,\n",
      "(backcast). Blocks are organized into stacks using doubly residual stacking principle. A stack may\n",
      "have layers with shared gbandgf. Forecasts are aggregated in hierarchical fashion. This enables\n",
      "building a very deep neural network with interpretable outputs.\n",
      "input window to a multiple of the forecast horizon H, and typical lengths of xin our setup range from\n",
      "2Hto7H. For the rest of the blocks, their inputs xℓare residual outputs of the previous blocks. Each\n",
      "block has two outputs: ˆyℓ, the block’s forward forecast of length H; andˆxℓ, the block’s best estimate\n",
      "ofxℓ, also known as the ‘backcast’, given the constraints on the functional space that the block can\n",
      "use to approximate signals.\n",
      "Internally, the basic building block consists of two parts. The ﬁrst part is a fully connected network\n",
      "that produces the forward θf\n",
      "ℓand the backward θb\n",
      "ℓpredictors of expansion coefﬁcients (again, note\n",
      "that the block index ℓis dropped for θb\n",
      "ℓ,θf\n",
      "ℓ,gb\n",
      "ℓ,gf\n",
      "ℓin Fig. 1 for brevity). The second part consists of\n",
      "the backward gb\n",
      "ℓand the forward gf\n",
      "ℓbasis layers that accept the respective forward θf\n",
      "ℓand backward\n",
      "θb\n",
      "ℓexpansion coefﬁcients, project them internally on the set of basis functions and produce the\n",
      "backcastˆxℓand the forecast outputs ˆyℓdeﬁned in the previous paragraph.\n",
      "The operation of the ﬁrst part of the ℓ-th block is described by the following equations:\n",
      "hℓ,1=FCℓ,1(xℓ),hℓ,2=FCℓ,2(hℓ,1),hℓ,3=FCℓ,3(hℓ,2),hℓ,4=FCℓ,4(hℓ,3).\n",
      "θb\n",
      "ℓ=LINEARb\n",
      "ℓ(hℓ,4),θf\n",
      "ℓ=LINEARf\n",
      "ℓ(hℓ,4).(1)\n",
      "Here LINEAR layer is simply a linear projection layer, i.e.θf\n",
      "ℓ=Wf\n",
      "ℓhℓ,4. The FClayer is a standard\n",
      "fully connected layer with RELUnon-linearity (Nair & Hinton, 2010), such that for FCℓ,1we have,\n",
      "for example: hℓ,1=RELU(Wℓ,1xℓ+bℓ,1). One task of this part of the architecture is to predict the\n",
      "forward expansion coefﬁcients θf\n",
      "ℓwith the ultimate goal of optimizing the accuracy of the partial\n",
      "forecastˆyℓby properly mixing the basis vectors supplied by gf\n",
      "ℓ. Additionally, this sub-network\n",
      "predicts backward expansion coefﬁcients θb\n",
      "ℓused by gb\n",
      "ℓto produce an estimate of xℓwith the ultimate\n",
      "goal of helping the downstream blocks by removing components of their input that are not helpful for\n",
      "forecasting.\n",
      "The second part of the network maps expansion coefﬁcients θf\n",
      "ℓandθb\n",
      "ℓto outputs via basis layers,\n",
      "ˆyℓ=gf\n",
      "ℓ(θf\n",
      "ℓ)andˆxℓ=gb\n",
      "ℓ(θb\n",
      "ℓ). Its operation is described by the following equations:\n",
      "ˆyℓ=dim(θf\n",
      "ℓ)\n",
      "∑\n",
      "i=1θf\n",
      "ℓ,ivf\n",
      "i,ˆxℓ=dim(θb\n",
      "ℓ)\n",
      "∑\n",
      "i=1θb\n",
      "ℓ,ivb\n",
      "i.\n",
      "3' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 2}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "Here vf\n",
      "iandvb\n",
      "iare forecast and backcast basis vectors, θf\n",
      "ℓ,iis the i-th element of θf\n",
      "ℓ. The function\n",
      "ofgb\n",
      "ℓandgf\n",
      "ℓis to provide sufﬁciently rich sets {vf\n",
      "i}dim(θf\n",
      "ℓ)\n",
      "i=1and{vb\n",
      "i}dim(θb\n",
      "ℓ)\n",
      "i=1such that their respective\n",
      "outputs can be represented adequately via varying expansion coefﬁcients θf\n",
      "ℓandθb\n",
      "ℓ. As shown below,\n",
      "gb\n",
      "ℓandgf\n",
      "ℓcan either be chosen to be learnable or can be set to speciﬁc functional forms to reﬂect\n",
      "certain problem-speciﬁc inductive biases in order to appropriately constrain the structure of outputs.\n",
      "Concrete examples of gb\n",
      "ℓandgf\n",
      "ℓare discussed in Section 3.3.\n",
      "3.2 D OUBLY RESIDUAL STACKING\n",
      "The classical residual network architecture adds the input of the stack of layers to its output before\n",
      "passing the result to the next stack (He et al., 2016). The DenseNet architecture proposed by Huang\n",
      "et al. (2017) extends this principle by introducing extra connections from the output of each stack to\n",
      "the input of every other stack that follows it. These approaches provide clear advantages in improving\n",
      "the trainability of deep architectures. Their disadvantage in the context of this work is that they result\n",
      "in network structures that are difﬁcult to interpret. We propose a novel hierarchical doubly residual\n",
      "topology depicted in Fig. 1 (middle and right). The proposed architecture has two residual branches,\n",
      "one running over backcast prediction of each layer and the other one is running over the forecast\n",
      "branch of each layer. Its operation is described by the following equations:\n",
      "xℓ=xℓ−1−ˆxℓ−1,ˆy=∑\n",
      "ℓˆyℓ.\n",
      "As previously mentioned, in the special case of the very ﬁrst block, its input is the model level\n",
      "input x,x1≡x. For all other blocks, the backcast residual branch xℓcan be thought of as running a\n",
      "sequential analysis of the input signal. Previous block removes the portion of the signal ˆxℓ−1that\n",
      "it can approximate well, making the forecast job of the downstream blocks easier. This structure\n",
      "also facilitates more ﬂuid gradient backpropagation. More importantly, each block outputs a partial\n",
      "forecastˆyℓthat is ﬁrst aggregated at the stack level and then at the overall network level, providing a\n",
      "hierarchical decomposition. The ﬁnal forecast ˆyis the sum of all partial forecasts. In a generic model\n",
      "context, when stacks are allowed to have arbitrary gb\n",
      "ℓandgf\n",
      "ℓfor each layer, this makes the network\n",
      "more transparent to gradient ﬂows. In a special situation of deliberate structure enforced in gb\n",
      "ℓandgf\n",
      "ℓshared over a stack, explained next, this has the critical importance of enabling interpretability via the\n",
      "aggregation of meaningful partial forecasts.\n",
      "3.3 I NTERPRETABILITY\n",
      "We propose two conﬁgurations of the architecture, based on the selection of gb\n",
      "ℓandgf\n",
      "ℓ. One of them\n",
      "is generic DL, the other one is augmented with certain inductive biases to be interpretable.\n",
      "Thegeneric architecture does not rely on TS-speciﬁc knowledge. We set gb\n",
      "ℓandgf\n",
      "ℓto be a linear\n",
      "projection of the previous layer output. In this case the outputs of block ℓare described as:\n",
      "ˆyℓ=Vf\n",
      "ℓθf\n",
      "ℓ+bf\n",
      "ℓ,ˆxℓ=Vb\n",
      "ℓθb\n",
      "ℓ+bb\n",
      "ℓ.\n",
      "The interpretation of this model is that the FC layers in the basic building block depicted in Fig. 1 learn\n",
      "the predictive decomposition of the partial forecast ˆyℓin the basis Vf\n",
      "ℓlearned by the network. Matrix\n",
      "Vf\n",
      "ℓhas dimensionality H×dim(θf\n",
      "ℓ). Therefore, the ﬁrst dimension of Vf\n",
      "ℓhas the interpretation of\n",
      "discrete time index in the forecast domain. The second dimension of the matrix has the interpretation\n",
      "of the indices of the basis functions, with θf\n",
      "ℓbeing the expansion coefﬁcients for this basis. Thus the\n",
      "columns of Vf\n",
      "ℓcan be thought of as waveforms in the time domain. Because no additional constraints\n",
      "are imposed on the form of Vf\n",
      "ℓ, the waveforms learned by the deep model do not have inherent\n",
      "structure (and none is apparent in our experiments). This leads to ˆyℓnot being interpretable.\n",
      "Theinterpretable architecture can be constructed by reusing the overall architectural approach in\n",
      "Fig. 1 and by adding structure to basis layers at stack level. Forecasting practitioners often use the\n",
      "decomposition of time series into trend and seasonality, such as those performed by the STL(Cleveland\n",
      "et al., 1990) and X13-ARIMA (U.S. Census Bureau, 2013). We propose to design the trend and\n",
      "seasonality decomposition into the model to make the stack outputs more easily interpretable. Note\n",
      "4' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 3}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "that for the generic model the notion of stack was not necessary and the stack level indexing was\n",
      "omitted for clarity. Now we will consider both stack level and block level indexing. For example, ˆys,ℓ\n",
      "will denote the partial forecast of block ℓwithin stack s.\n",
      "Trend model. A typical characteristic of trend is that most of the time it is a monotonic function, or\n",
      "at least a slowly varying function. In order to mimic this behaviour we propose to constrain gb\n",
      "s,ℓand\n",
      "gf\n",
      "s,ℓto be a polynomial of small degree p, a function slowly varying across forecast window:\n",
      "ˆys,ℓ=p\n",
      "∑\n",
      "i=0θf\n",
      "s,ℓ,iti. (2)\n",
      "Here time vector t= [0,1,2,..., H−2,H−1]T/His deﬁned on a discrete grid running from 0 to\n",
      "(H−1)/H, forecasting Hsteps ahead. Alternatively, the trend forecast in matrix form will then be:\n",
      "ˆytr\n",
      "s,ℓ=Tθf\n",
      "s,ℓ,\n",
      "where θf\n",
      "s,ℓare polynomial coefﬁcients predicted by a FC network of layer ℓof stack sdescribed by\n",
      "equations (1); and T= [1,t,...,tp]is the matrix of powers of t. Ifpis low, e.g. 2 or 3, it forces ˆytr\n",
      "s,ℓ\n",
      "to mimic trend.\n",
      "Seasonality model. Typical characteristic of seasonality is that it is a regular, cyclical, recurring\n",
      "ﬂuctuation. Therefore, to model seasonality, we propose to constrain gb\n",
      "s,ℓandgf\n",
      "s,ℓto belong to the\n",
      "class of periodic functions, i.e.yt=yt−∆, where ∆is a seasonality period. A natural choice for the\n",
      "basis to model periodic function is the Fourier series:\n",
      "ˆys,ℓ=⌊H/2−1⌋\n",
      "∑\n",
      "i=0θf\n",
      "s,ℓ,icos(2πit)+θf\n",
      "s,ℓ,i+⌊H/2⌋sin(2πit), (3)\n",
      "The seasonality forecast will then have the matrix form as follows:\n",
      "ˆyseas\n",
      "s,ℓ=Sθf\n",
      "s,ℓ,\n",
      "where θf\n",
      "s,ℓare Fourier coefﬁcients predicted by a FC network of layer ℓof stack sdescribed by\n",
      "equations (1); and S= [1,cos(2πt),...cos(2π⌊H/2−1⌋t)),sin(2πt),..., sin(2π⌊H/2−1⌋t))]is the\n",
      "matrix of sinusoidal waveforms. The forecast ˆyseas\n",
      "s,ℓis then a periodic function mimicking typical\n",
      "seasonal patterns.\n",
      "The overall interpretable architecture consists of two stacks: the trend stack is followed by the\n",
      "seasonality stack. The doubly residual stacking combined with the forecast/backcast principle result\n",
      "in (i) the trend component being removed from the input window xbefore it is fed into the seasonality\n",
      "stack and (ii) the partial forecasts of trend and seasonality are available as separate interpretable\n",
      "outputs. Structurally, each of the stacks consists of several blocks connected with residual connections\n",
      "as depicted in Fig. 1 and each of them shares its respective, non-learnable gb\n",
      "s,ℓandgf\n",
      "s,ℓ. The number\n",
      "of blocks is 3 for both trend and seasonality. We found that on top of sharing gb\n",
      "s,ℓandgf\n",
      "s,ℓ, sharing all\n",
      "the weights across blocks in a stack resulted in better validation performance.\n",
      "3.4 E NSEMBLING\n",
      "Ensembling is used by all the top entries in the M4-competition. We rely on ensembling as well\n",
      "to be comparable. We found that ensembling is a much more powerful regularization technique\n",
      "than the popular alternatives, e.g. dropout or L2-norm penalty. The addition of those methods\n",
      "improved individual models, but was hurting the performance of the ensemble. The core property of\n",
      "an ensemble is diversity. We build an ensemble using several sources of diversity. First, the ensemble\n",
      "models are ﬁt on three different metrics: sMAPE,MASE and MAPE , a version of sMAPE that has only\n",
      "the ground truth value in the denominator. Second, for every horizon H, individual models are trained\n",
      "on input windows of different length: 2H,3H,..., 7H, for a total of six window lengths. Thus the\n",
      "overall ensemble exhibits a multi-scale aspect. Finally, we perform a bagging procedure (Breiman,\n",
      "1996) by including models trained with different random initializations. We use 180 total models to\n",
      "report results on the test set (please refer to Appendix B for the ablation of ensemble size). We use\n",
      "the median as ensemble aggregation function.\n",
      "5' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 4}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "Table 1: Performance on the M4, M3, TOURISM test sets, aggregated over each dataset. Evaluation\n",
      "metrics are speciﬁed for each dataset; lower values are better. The number of time series in each\n",
      "dataset is provided in brackets.\n",
      "M4 Average (100,000) M3 Average (3,003) TOURISM Average (1,311)\n",
      "sMAPE OWA sMAPE MAPE\n",
      "Pure ML 12.894 0.915 Comb S-H-D 13.52 ETS 20.88\n",
      "Statistical 11.986 0.861 ForecastPro 13.19 Theta 20.88\n",
      "ProLogistica 11.845 0.841 Theta 13.01 ForePro 19.84\n",
      "ML/TS combination 11.720 0.838 DOTM 12.90 Stratometrics 19.52\n",
      "DL/TS hybrid 11.374 0.821 EXP 12.71 LeeCBaker 19.35\n",
      "N-BEATS-G 11.168 0.797 12.47 18.47\n",
      "N-BEATS-I 11.174 0.798 12.43 18.97\n",
      "N-BEATS-I+G 11.135 0.795 12.37 18.52\n",
      "4 R ELATED WORK\n",
      "The approaches to TS forecasting can be split in a few distinct categories. The statistical model-\n",
      "ing approaches based on exponential smoothing and its different ﬂavors are well established and\n",
      "are often considered a default choice in the industry (Holt, 1957; 2004; Winters, 1960). More\n",
      "advanced variations of exponential smoothing include the winner of M3 competition, the Theta\n",
      "method (Assimakopoulos & Nikolopoulos, 2000) that decomposes the forecast into several theta-lines\n",
      "and statistically combines them. The pinnacle of the statistical approach encapsulates ARIMA,\n",
      "auto-ARIMA and in general, the uniﬁed state-space modeling approach, that can be used to ex-\n",
      "plain and analyze all of the approaches mentioned above (see Hyndman & Khandakar (2008) for\n",
      "an overview). More recently, ML/TS combination approaches started inﬁltrating the domain with\n",
      "great success, showing promising results by using the outputs of statistical engines as features. In\n",
      "fact, 2 out of top-5 entries in the M4 competition are approaches of this type, including the second\n",
      "entry (Montero-Manso et al., 2019). The second entry computes the outputs of several statistical\n",
      "methods on the M4 dataset and combines them using gradient boosted tree (Chen & Guestrin, 2016).\n",
      "Somewhat independently, the work in the modern deep learning TS forecasting developed based on\n",
      "variations of recurrent neural networks (Flunkert et al., 2017; Rangapuram et al., 2018b; Toubeau\n",
      "et al., 2019; Zia & Razzaq, 2018) being largely dominated by the electricity load forecasting in the\n",
      "multi-variate setup. A few earlier works explored the combinations of recurrent neural networks\n",
      "with dilation, residual connections and attention (Chang et al., 2017; Kim et al., 2017; Qin et al.,\n",
      "2017). These served as a basis for the winner of the M4 competition (Smyl, 2020). The winning\n",
      "entry combines a Holt-Winters style seasonality model with its parameters ﬁtted to a given TS via\n",
      "gradient descent and a unique combination of dilation/residual/attention approaches for each forecast\n",
      "horizon. The resulting model is a hybrid model that architecturally heavily relies on a time-series\n",
      "engine. It is hand crafted to each speciﬁc horizon of M4, making this approach hard to generalize to\n",
      "other datasets.\n",
      "5 E XPERIMENTAL RESULTS\n",
      "Our key empirical results based on aggregate performance metrics over several datasets—M4 (M4\n",
      "Team, 2018b; Makridakis et al., 2018b), M3 (Makridakis & Hibon, 2000; Makridakis et al., 2018a)\n",
      "and TOURISM (Athanasopoulos et al., 2011)—appear in Table 1. More detailed descriptions of the\n",
      "datasets are provided in Section 5.1 and Appendix A. For each dataset, we compare our results with\n",
      "best 5 entries for this dataset reported in the literature, according to the customary metrics speciﬁc to\n",
      "each dataset (M4: OWA andsMAPE , M3: sMAPE ,TOURISM :MAPE ). More granular dataset-speciﬁc\n",
      "results with data splits over forecast horizons and types of time series appear in respective appendices\n",
      "(M4: Appendix C.1; M3: Appendix C.2; TOURISM : Appendix C.3).\n",
      "In Table 1, we study the performance of two N-BEATS conﬁgurations: generic (N-BEATS-G) and\n",
      "interpretable (N-BEATS-I), as well as N-BEATS-I+G (ensemble of all models from N-BEATS-G and\n",
      "N-BEATS-I). On M4 dataset , we compare against 5 representatives from the M4 competition (Makri-\n",
      "6' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 5}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "dakis et al., 2018b): each best in their respective model class. Pure ML is the submission by B. Trotta,\n",
      "the best entry among the 6 pure ML models. Statistical is the best pure statistical model by N.Z.\n",
      "Legaki and K. Koutsouri. ML/TS combination is the model by P. Montero-Manso, T. Talagala, R.J.\n",
      "Hyndman and G. Athanasopoulos, second best entry, gradient boosted tree over a few statistical time\n",
      "series models. ProLogistica is the third entry in M4 based on the weighted ensemble of statistical\n",
      "methods. Finally, DL/TS hybrid is the winner of M4 competition (Smyl, 2020). On the M3 dataset ,\n",
      "we compare against the Theta method (Assimakopoulos & Nikolopoulos, 2000), the winner of M3;\n",
      "DOTA , a dynamically optimized Theta model (Fiorucci et al., 2016); EXP, the most resent statistical\n",
      "approach and the previous state-of-the-art on M3 (Spiliotis et al., 2019); as well as ForecastPro , an\n",
      "off-the-shelf forecasting software that is based on model selection between exponential smoothing,\n",
      "ARIMA and moving average (Athanasopoulos et al., 2011; Assimakopoulos & Nikolopoulos, 2000).\n",
      "On the TOURISM dataset , we compare against 3 statistical benchmarks (Athanasopoulos et al.,\n",
      "2011): ETS, exponential smoothing with cross-validated additive/multiplicative model; Theta method;\n",
      "ForePro , same as ForecastPro in M3; as well as top 2 entries from the TOURISM Kaggle competi-\n",
      "tion (Athanasopoulos & Hyndman, 2011): Stratometrics , an unknown technique; LeeCBaker (Baker\n",
      "& Howard, 2011), a weighted combination of Naïve, linear trend model, and exponentially weighted\n",
      "least squares regression trend.\n",
      "According to Table 1, N-BEATS demonstrates state-of-the-art performance on three challenging\n",
      "non-overlapping datasets containing time series from very different domains, sampling frequencies\n",
      "and seasonalities. As an example, on M4 dataset, the OWA gap between N-BEATS and the M4\n",
      "winner ( 0.821−0.795=0.026) is greater than the gap between the M4 winner and the second entry\n",
      "(0.838−0.821=0.017). Generic N-BEATS model uses as little prior knowledge as possible, with\n",
      "no feature engineering, no scaling and no internal architectural components that may be considered\n",
      "TS-speciﬁc. Thus the result in Table 1 leads us to the conclusion that DL does not need support\n",
      "from the statistical approaches or hand-crafted feature engineering and domain knowledge to perform\n",
      "extremely well on a wide array of TS forecasting tasks. On top of that, the proposed general\n",
      "architecture performs very well on three different datasets outperforming a wide variety of models,\n",
      "both generic and manually crafted to respective dataset, including the winner of M4, a model\n",
      "architecturally adjusted by hand to each forecast-horizon subset of the M4 data.\n",
      "5.1 D ATASETS\n",
      "M4(M4 Team, 2018b; Makridakis et al., 2018b) is the latest in an inﬂuential series of forecasting\n",
      "competitions organized by Spyros Makridakis since 1982 (Makridakis et al., 1982). The 100k-series\n",
      "dataset is large and diverse, consisting of data frequently encountered in business, ﬁnancial and\n",
      "economic forecasting, and sampling frequencies ranging from hourly to yearly. A table with summary\n",
      "statistics is presented in Appendix A.1, showing wide variability in TS characteristics.\n",
      "M3(Makridakis & Hibon, 2000) is similar in its composition to M4, but has a smaller overall scale\n",
      "(3003 time series total vs. 100k in M4). A table with summary statistics is presented in Appendix A.2.\n",
      "Over the past 20 years, this dataset has supported signiﬁcant efforts in the design of more optimal\n",
      "statistical models, e.g. Theta and its variants (Assimakopoulos & Nikolopoulos, 2000; Fiorucci et al.,\n",
      "2016; Spiliotis et al., 2019). Furthermore, a recent publication (Makridakis et al., 2018a) based on a\n",
      "subset of M3 presented evidence that ML models are inferior to the classical statistical models.\n",
      "TOURISM (Athanasopoulos et al., 2011) dataset was released as part of the respective Kaggle\n",
      "competition conducted by Athanasopoulos & Hyndman (2011). The data include monthly, quarterly\n",
      "and yearly series supplied by both governmental tourism organizations (e.g. Tourism Australia, the\n",
      "Hong Kong Tourism Board and Tourism New Zealand) as well as various academics, who had used\n",
      "them in previous studies. A table with summary statistics is presented in Appendix A.3.\n",
      "5.2 T RAINING METHODOLOGY\n",
      "We split each dataset into train, validation and test subsets. The test subset is the standard test set\n",
      "previously deﬁned for each dataset (M4 Team, 2018a; Makridakis & Hibon, 2000; Athanasopoulos\n",
      "et al., 2011). The validation and train subsets for each dataset are obtained by splitting their full train\n",
      "sets at the boundary of the last horizon of each time series. We use the train and validation subsets to\n",
      "tune hyperparameters. Once the hyperparameters are determined, we train the model on the full train\n",
      "set and report results on the test set. Please refer to Appendix D for detailed hyperparameter settings\n",
      "7' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 6}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "at the block level. N-BEATS is implemented and trained in Tensorﬂow (Abadi et al., 2015). We\n",
      "share parameters of the network across horizons, therefore we train one model per horizon for each\n",
      "dataset. If every time series is interpreted as a separate task, this can be linked back to the multitask\n",
      "learning and furthermore to meta-learning (see discussion in Section 6), in which a neural network\n",
      "is regularized by learning on multiple tasks to improve generalization. We would like to stress that\n",
      "models for different horizons and datasets reuse the same architecture. Architectural hyperparameters\n",
      "(width, number of layers, number of stacks, etc.) are ﬁxed to the same values across horizons and\n",
      "across datasets (see Appendix D). The fact that we can reuse architecture and even hyperparameters\n",
      "across horizons indicates that the proposed architecture design generalizes well across time series of\n",
      "different nature. The same architecture is successfully trained on the M4 Monthly subset with 48k\n",
      "time series and the M3 Others subset with 174 time series. This is a much stronger result than e.g.the\n",
      "result of S. Smyl (Makridakis et al., 2018b) who had to use very different architectures hand crafted\n",
      "for different horizons.\n",
      "To update network parameters for one horizon, we sample train batches of ﬁxed size 1024. We pick\n",
      "1024 TS ids from this horizon, uniformly at random with replacement. For each selected TS id we\n",
      "pick a random forecast point from the historical range of length LHimmediately preceding the last\n",
      "point in the train part of the TS. LHis a cross-validated hyperparameter. We observed that for subsets\n",
      "with large number of time series it tends to be smaller and for subsets with smaller number of time\n",
      "series it tends to be larger. For example, in massive Yearly, Monthly, Quarterly subsets of M4 LHis\n",
      "equal to 1.5; and in moderate to small Weekly, Daily, Hourly subsets of M4 LHis equal to 10. Given\n",
      "a sampled forecast point, we set one horizon worth of points following it to be the target forecast\n",
      "window yand we set the history of points of one of lengths 2H,3H,..., 7Hpreceding it to be the\n",
      "input xto the network. We use the Adam optimizer with default settings and initial learning rate\n",
      "0.001. While optimising the ensemble members relying on the minimization of sMAPE metric, we\n",
      "stop the gradient ﬂows in the denominator to make training numerically stable. The neural network\n",
      "training is run with early stopping and the number of batches is determined on the validation set. The\n",
      "GPU based training of one ensemble member for entire M4 dataset takes between 30 min and 2 hours\n",
      "depending on neural network settings and hardware.\n",
      "5.3 I NTERPRETABILITY RESULTS\n",
      "Fig. 2 studies the outputs of the proposed model in the generic and the interpretable conﬁgurations.\n",
      "As discussed in Section 3.3, to make the generic architecture presented in Fig. 1 interpretable, we\n",
      "constrain gθin the ﬁrst stack to have the form of polynomial (2)while the second one has the form\n",
      "of Fourier basis (3). Furthermore, we use the outputs of the generic conﬁguration of N-BEATS as\n",
      "control group (the generic model of 30 residual blocks depicted in Fig. 1 is divided into two stacks)\n",
      "and we plot both generic (sufﬁx “-G”) and interpretable (sufﬁx “-I”) stack outputs side by side in\n",
      "Fig. 2. The outputs of generic model are arbitrary and non-interpretable: either trend or seasonality\n",
      "or both of them are present at the output of both stacks. The magnitude of the output (peak-to-peak)\n",
      "is generally smaller at the output of the second stack. The outputs of the interpretable model exhibit\n",
      "distinct properties: the trend output is monotonic and slowly moving, the seasonality output is\n",
      "regular, cyclical and has recurring ﬂuctuations. The peak-to-peak magnitude of the seasonality output\n",
      "is signiﬁcantly larger than that of the trend, if signiﬁcant seasonality is present in the time series.\n",
      "Similarly, the peak-to-peak magnitude of trend output tends to be small when no obvious trend\n",
      "is present in the ground truth signal. Thus the proposed interpretable architecture decomposes its\n",
      "forecast into two distinct components. Our conclusion is that the outputs of the DL model can be\n",
      "made interpretable by encoding a sensible inductive bias in the architecture. Table 1 conﬁrms that\n",
      "this does not result in performance drop.\n",
      "6 D ISCUSSION : CONNECTIONS TO META-LEARNING\n",
      "Meta-learning deﬁnes an inner learning procedure and an outer learning procedure . The inner\n",
      "learning procedure is parameterized, conditioned or otherwise inﬂuenced by the outer learning\n",
      "procedure (Bengio et al., 1991). The prototypical inner vs. outer learning is individual learning in\n",
      "the lifetime of an animal vs. evolution of the inner learning procedure itself over many generations\n",
      "of individuals. To see the two levels, it often helps to refer to two sets of parameters, the inner\n",
      "parameters (e.g. synaptic weights) which are modiﬁed inside the inner learning procedure, and the\n",
      "8' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 7}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "0 1 2 3 4 5\n",
      "t0.80.91.0\n",
      "ACTUAL\n",
      "FORECAST-I\n",
      "FORECAST-G\n",
      "0 1 2 3 4 5\n",
      "t0.800.850.90STACK1-G\n",
      "0 1 2 3 4 5\n",
      "t0.0250.0500.075STACK2-G\n",
      "0 1 2 3 4 5\n",
      "t0.800.850.900.95\n",
      "STACK1-I\n",
      "0 1 2 3 4 5\n",
      "t0.020.030.040.05STACK2-I\n",
      "0 2 4 6\n",
      "t0.850.900.951.00\n",
      "ACTUAL\n",
      "FORECAST-I\n",
      "FORECAST-G\n",
      "0 2 4 6\n",
      "t0.860.880.90STACK1-G\n",
      "0 2 4 6\n",
      "t0.025\n",
      "0.0000.0250.050\n",
      "STACK2-G\n",
      "0 2 4 6\n",
      "t0.880.890.90STACK1-I\n",
      "0 2 4 6\n",
      "t0.05\n",
      "0.000.05STACK2-I\n",
      "0 5 10 15\n",
      "t0.40.60.81.0\n",
      "ACTUAL\n",
      "FORECAST-I\n",
      "FORECAST-G\n",
      "0 5 10 15\n",
      "t0.80.9 STACK1-G\n",
      "0 5 10 15\n",
      "t0.1\n",
      "0.0\n",
      "STACK2-G\n",
      "0 5 10 15\n",
      "t0.850.90STACK1-I\n",
      "0 5 10 15\n",
      "t0.3\n",
      "0.2\n",
      "0.1\n",
      "0.0STACK2-I\n",
      "0 2 4 6 8 10 12\n",
      "t0.60.81.0\n",
      "ACTUAL\n",
      "FORECAST-I\n",
      "FORECAST-G\n",
      "0 2 4 6 8 10 12\n",
      "t0.650.700.750.80\n",
      "STACK1-G\n",
      "0 2 4 6 8 10 12\n",
      "t0.0000.0250.0500.075\n",
      "STACK2-G\n",
      "0 2 4 6 8 10 12\n",
      "t0.650.700.750.80STACK1-I\n",
      "0 2 4 6 8 10 12\n",
      "t0.000.020.04STACK2-I\n",
      "0.0 2.5 5.0 7.5 10.0 12.5\n",
      "t0.960.981.00\n",
      "ACTUAL\n",
      "FORECAST-I\n",
      "FORECAST-G\n",
      "0.0 2.5 5.0 7.5 10.0 12.5\n",
      "t0.9740.976STACK1-G\n",
      "0.0 2.5 5.0 7.5 10.0 12.5\n",
      "t0.002\n",
      "0.001\n",
      "STACK2-G\n",
      "0.0 2.5 5.0 7.5 10.0 12.5\n",
      "t0.9740.976STACK1-I\n",
      "0.0 2.5 5.0 7.5 10.0 12.5\n",
      "t0.0003\n",
      "0.0002\n",
      "0.0001\n",
      "STACK2-I\n",
      "0 10 20 30 40\n",
      "t0.250.500.751.00\n",
      "ACTUAL\n",
      "FORECAST-I\n",
      "FORECAST-G\n",
      "(a) Combined\n",
      "0 10 20 30 40\n",
      "t0.20.40.6\n",
      "STACK1-G (b) Stack1-G\n",
      "0 10 20 30 40\n",
      "t0.02\n",
      "0.00\n",
      "STACK2-G (c) Stack2-G\n",
      "0 10 20 30 40\n",
      "t0.360.380.40\n",
      "STACK1-I (d) StackT-I\n",
      "0 10 20 30 40\n",
      "t0.2\n",
      "0.00.2\n",
      "STACK2-I (e) StackS-I\n",
      "Figure 2: The outputs of generic and the interpretable conﬁgurations, M4 dataset. Each row is one\n",
      "time series example per data frequency, top to bottom (Yearly: id Y3974, Quarterly: id Q11588,\n",
      "Monthly: id M19006, Weekly: id W246, Daily: id D404, Hourly: id H344). The magnitudes in a row\n",
      "are normalized by the maximal value of the actual time series for convenience. Column (a) shows the\n",
      "actual values (ACTUAL), the generic model forecast (FORECAST-G) and the interpretable model\n",
      "forecast (FORECAST-I). Columns (b) and (c) show the outputs of stacks 1 and 2 of the generic model,\n",
      "respectively; FORECAST-G is their summation. Columns (d) and (e) show the output of the Trend\n",
      "and the Seasonality stacks of the interpretable model, respectively; FORECAST-I is their summation.\n",
      "outer parameters or meta-parameters (e.g. genes) which get modiﬁed only in the outer learning\n",
      "procedure.\n",
      "N-BEATS can be cast as an instance of meta-learning by drawing the following parallels. The outer\n",
      "learning procedure is encapsulated in the parameters of the whole network, learned by gradient\n",
      "descent. The inner learning procedure is encapsulated in the set of basic building blocks and modiﬁes\n",
      "the expansion coefﬁcients θfthat basis gftakes as inputs. The inner learning proceeds through a\n",
      "sequence of stages, each corresponding to a block within the stack of the architecture. Each of the\n",
      "blocks can be thought of as performing the equivalent of an update step which gradually modiﬁes\n",
      "the expansion coefﬁcients θfwhich eventually feed into gfin each block (which get added together\n",
      "to form the ﬁnal prediction). The inner learning procedure takes a single history from a piece of a\n",
      "TS and sees that history as a training set. It produces forward expansion coefﬁcients θf(see Fig. 1),\n",
      "which parametrically map inputs to predictions. In addition, each preceding block modiﬁes the input\n",
      "to the next block by producing backward expansion coefﬁcients θb, thus conditioning the learning\n",
      "and the output of the next block. In the case of the interpretable model, the meta-parameters are only\n",
      "in the FC layers because the gf’s are ﬁxed. In the case of the generic model, the meta-parameters\n",
      "also include the V’s which deﬁne the gfnon-parametrically. This point of view is further reinforced\n",
      "by the results of the ablation study reported in Appendix B showing that increasing the number of\n",
      "blocks in the stack, as well as the number of stacks improves generalization performance, and can be\n",
      "interpreted as more iterations of the inner learning procedure.\n",
      "9' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 8}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "7 C ONCLUSIONS\n",
      "We proposed and empirically validated a novel architecture for univariate TS forecasting. We showed\n",
      "that the architecture is general, ﬂexible and it performs well on a wide array of TS forecasting prob-\n",
      "lems. We applied it to three non-overlapping challenging competition datasets: M4, M3 and TOURISM\n",
      "and demonstrated state-of-the-art performance in two conﬁgurations: generic and interpretable. This\n",
      "allowed us to validate two important hypotheses: (i) the generic DL approach performs exceptionally\n",
      "well on heterogeneous univariate TS forecasting problems using no TS domain knowledge, (ii) it is\n",
      "viable to additionally constrain a DL model to force it to decompose its forecast into distinct human\n",
      "interpretable outputs. We also demonstrated that the DL models can be trained on multiple time series\n",
      "in a multi-task fashion, successfully transferring and sharing individual learnings. We speculate that\n",
      "N-BEATS’s performance can be attributed in part to it carrying out a form of meta-learning, a deeper\n",
      "investigation of which should be the subject of future work.\n",
      "REFERENCES\n",
      "Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.\n",
      "Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew\n",
      "Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath\n",
      "Kudlur, Josh Levenberg, Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike\n",
      "Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent\n",
      "Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg,\n",
      "Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning\n",
      "on heterogeneous systems, 2015. URL http://tensorflow.org/ . Software available from\n",
      "tensorﬂow.org.\n",
      "V . Assimakopoulos and K. Nikolopoulos. The theta model: a decomposition approach to forecasting.\n",
      "International Journal of Forecasting , 16(4):521–530, 2000.\n",
      "George Athanasopoulos and Rob J. Hyndman. The value of feedback in forecasting competitions.\n",
      "International Journal of Forecasting , 27(3):845–849, 2011.\n",
      "George Athanasopoulos, Rob J. Hyndman, Haiyan Song, and Doris C. Wu. The tourism forecasting\n",
      "competition. International Journal of Forecasting , 27(3):822–844, 2011.\n",
      "Lee C. Baker and Jeremy Howard. Winning methods for forecasting tourism time series. International\n",
      "Journal of Forecasting , 27(3):850–852, 2011.\n",
      "Yoshua Bengio, Samy Bengio, and Jocelyn Cloutier. Learning a synaptic learning rule. In Proceedings\n",
      "of the International Joint Conference on Neural Networks , pp. II–A969, Seattle, USA, 1991.\n",
      "Christoph Bergmeir, Rob J. Hyndman, and José M. Benítez. Bagging exponential smoothing methods\n",
      "using STL decomposition and Box–Cox transformation. International Journal of Forecasting , 32\n",
      "(2):303–312, 2016.\n",
      "Leo Breiman. Bagging predictors. Machine Learning , 24(2):123–140, Aug 1996.\n",
      "Phil Brierley. Winning methods for forecasting seasonal tourism time series. International Journal of\n",
      "Forecasting , 27(3):853–854, 2011.\n",
      "Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael\n",
      "Witbrock, Mark A Hasegawa-Johnson, and Thomas S Huang. Dilated recurrent neural networks.\n",
      "InNIPS , pp. 77–87, 2017.\n",
      "Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In ACM SIGKDD , pp.\n",
      "785–794, 2016.\n",
      "Robert B. Cleveland, William S. Cleveland, Jean E. McRae, and Irma Terpenning. STL: A seasonal-\n",
      "trend decomposition procedure based on Loess (with discussion). Journal of Ofﬁcial Statistics , 6:\n",
      "3–73, 1990.\n",
      "Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.ics.\n",
      "uci.edu/ml .\n",
      "10' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 9}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "Jose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, and Anne B. Koehler.\n",
      "Models for optimising the Theta method and their relationship to state space models. International\n",
      "Journal of Forecasting , 32(4):1151–1161, 2016.\n",
      "Valentin Flunkert, David Salinas, and Jan Gasthaus. DeepAR: Probabilistic forecasting with autore-\n",
      "gressive recurrent networks. CoRR , abs/1704.04110, 2017.\n",
      "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\n",
      "recognition. In CVPR , pp. 770–778. IEEE Computer Society, 2016.\n",
      "C. C. Holt. Forecasting trends and seasonals by exponentially weighted averages. Technical Report\n",
      "ONR memorandum no. 5, Carnegie Institute of Technology, Pittsburgh, PA, 1957.\n",
      "Charles C. Holt. Forecasting seasonals and trends by exponentially weighted moving averages.\n",
      "International Journal of Forecasting , 20(1):5–10, 2004.\n",
      "Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected\n",
      "convolutional networks. In CVPR , pp. 2261–2269. IEEE Computer Society, 2017.\n",
      "Rob Hyndman and Anne B. Koehler. Another look at measures of forecast accuracy. International\n",
      "Journal of Forecasting , 22(4):679–688, 2006.\n",
      "Rob J Hyndman and Yeasmin Khandakar. Automatic time series forecasting: the forecast package\n",
      "for R. Journal of Statistical Software , 26(3):1–22, 2008.\n",
      "Chaman L. Jain. Answers to your forecasting questions. Journal of Business Forecasting , 36, Spring\n",
      "2017.\n",
      "Kenneth B. Kahn. How to measure the impact of a forecast error on an enterprise? The Journal of\n",
      "Business Forecasting Methods & Systems , 22(1), Spring 2003.\n",
      "Jaeyoung Kim, Mostafa El-Khamy, and Jungwon Lee. Residual lstm: Design of a deep recurrent\n",
      "architecture for distant speech recognition. In Interspeech 2017 , pp. 1591–1595, 2017.\n",
      "M4 Team. M4 dataset, 2018a. URL https://github.com/M4Competition/M4-methods/tree/\n",
      "master/Dataset .\n",
      "M4 Team. M4 competitor’s guide: prizes and rules, 2018b. URL www.m4.unic.ac.cy/\n",
      "wp-content/uploads/2018/03/M4-CompetitorsGuide.pdf .\n",
      "S Makridakis, E Spiliotis, and V Assimakopoulos. Statistical and machine learning forecasting\n",
      "methods: Concerns and ways forward. PLoS ONE , 13(3), 2018a.\n",
      "Spyros Makridakis and Michèle Hibon. The M3-Competition: results, conclusions and implications.\n",
      "International Journal of Forecasting , 16(4):451–476, 2000.\n",
      "Spyros Makridakis, A Andersen, Robert Carbone, Robert Fildes, Michele Hibon, Rudolf\n",
      "Lewandowski, Joseph Newton, Emanuel Parzen, and Robert Winkler. The accuracy of extrapo-\n",
      "lation (time series) methods: Results of a forecasting competition. Journal of forecasting , 1(2):\n",
      "111–153, 1982.\n",
      "Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The M4-Competition:\n",
      "Results, ﬁndings, conclusion and way forward. International Journal of Forecasting , 34(4):\n",
      "802–808, 2018b.\n",
      "Pablo Montero-Manso, George Athanasopoulos, Rob J Hyndman, and Thiyanga S Talagala.\n",
      "FFORMA: Feature-based Forecast Model Averaging. International Journal of Forecasting , 2019.\n",
      "to appear.\n",
      "Vinod Nair and Geoffrey E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In\n",
      "ICML , pp. 807–814, 2010.\n",
      "Yao Qin, Dongjin Song, Haifeng Chen, Wei Cheng, Guofei Jiang, and Garrison W. Cottrell. A\n",
      "dual-stage attention-based recurrent neural network for time series prediction. In IJCAI-17 , pp.\n",
      "2627–2633, 2017.\n",
      "11' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 10}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "Syama Sundar Rangapuram, Matthias Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and Tim\n",
      "Januschowski. Deep state space models for time series forecasting. In NeurIPS , 2018a.\n",
      "Syama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and\n",
      "Tim Januschowski. Deep state space models for time series forecasting. In NeurIPS 31 , pp.\n",
      "7785–7794, 2018b.\n",
      "Slawek Smyl. A hybrid method of exponential smoothing and recurrent neural networks for time\n",
      "series forecasting. International Journal of Forecasting , 36(1):75 – 85, 2020.\n",
      "Slawek Smyl and Karthik Kuber. Data preprocessing and augmentation for multiple short time series\n",
      "forecasting with recurrent neural networks. In 36th International Symposium on Forecasting , 2016.\n",
      "Evangelos Spiliotis, Vassilios Assimakopoulos, and Konstantinos Nikolopoulos. Forecasting with a\n",
      "hybrid method utilizing data smoothing, a variation of the theta method and shrinkage of seasonal\n",
      "factors. International Journal of Production Economics , 209:92–102, 2019.\n",
      "A. A. Syntetos, J. E. Boylan, and J. D. Croston. On the categorization of demand patterns. Journal of\n",
      "the Operational Research Society , 56(5):495–503, 2005.\n",
      "J. Toubeau, J. Bottieau, F. Vallée, and Z. De Grève. Deep learning-based multivariate probabilistic\n",
      "forecasting for short-term scheduling in power markets. IEEE Transactions on Power Systems , 34\n",
      "(2):1203–1215, March 2019.\n",
      "U.S. Census Bureau. Reference manual for the X-13ARIMA-SEATS Program, version 1.0, 2013.\n",
      "URL http://www.census.gov/ts/x13as/docX13AS.pdf .\n",
      "Yuyang Wang, Alex Smola, Danielle C. Maddix, Jan Gasthaus, Dean Foster, and Tim Januschowski.\n",
      "Deep factors for forecasting. In ICML , 2019.\n",
      "Peter R. Winters. Forecasting sales by exponentially weighted moving averages. Management\n",
      "Science , 6(3):324–342, 1960.\n",
      "Hsiang-Fu Yu, Nikhil Rao, and Inderjit S. Dhillon. Temporal regularized matrix factorization for\n",
      "high-dimensional time series prediction. In NIPS , 2016.\n",
      "Tehseen Zia and Saad Razzaq. Residual recurrent highway networks for learning deep sequence\n",
      "prediction models. Journal of Grid Computing , Jun 2018.\n",
      "12' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 11}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "Table 2: Composition of the M4 dataset: the number of time series based on their sampling frequency\n",
      "and type.\n",
      "Frequency / Horizon\n",
      "Type Yearly/6 Qtly/8 Monthly/18 Wkly/13 Daily/14 Hrly/48 Total\n",
      "Demographic 1,088 1,858 5,728 24 10 0 8,708\n",
      "Finance 6,519 5,305 10,987 164 1,559 0 24,534\n",
      "Industry 3,716 4,637 10,017 6 422 0 18,798\n",
      "Macro 3,903 5,315 10,016 41 127 0 19,402\n",
      "Micro 6,538 6,020 10,975 112 1,476 0 25,121\n",
      "Other 1,236 865 277 12 633 414 3,437\n",
      "Total 23,000 24,000 48,000 359 4,227 414 100,000\n",
      "Min. Length 19 24 60 93 107 748\n",
      "Max. Length 841 874 2812 2610 9933 1008\n",
      "Mean Length 37.3 100.2 234.3 1035.0 2371.4 901.9\n",
      "SD Length 24.5 51.1 137.4 707.1 1756.6 127.9\n",
      "% Smooth 82% 89% 94% 84% 98% 83%\n",
      "% Erratic 18% 11% 6% 16% 2% 17%\n",
      "A D ATASET DETAILS\n",
      "A.1 M4 D ATASET DETAILS\n",
      "Table 2 outlines the composition of the M4 dataset across domains and forecast horizons by listing the\n",
      "number of time series based on their frequency and type (M4 Team, 2018b). The M4 dataset is large\n",
      "and diverse: all forecast horizons are composed of heterogeneous time series types (with exception of\n",
      "Hourly) frequently encountered in business, ﬁnancial and economic forecasting. Summary statistics\n",
      "on series lengths are also listed, showing wide variability therein, as well as a characterization ( smooth\n",
      "vserratic ) that follows Syntetos et al. (2005), and is based on the squared coefﬁcient of variation of\n",
      "the series. All series have positive observed values at all time-steps; as such, none can be considered\n",
      "intermittent orlumpy per Syntetos et al. (2005).\n",
      "A.2 M3 D ATASET DETAILS\n",
      "Table 3 outlines the composition of the M3 dataset across domains and forecast horizons by listing\n",
      "the number of time series based on their frequency and type (Makridakis & Hibon, 2000). The\n",
      "M3 is smaller than the M4, but it is still large and diverse: all forecast horizons are composed\n",
      "of heterogeneous time series types frequently encountered in business, ﬁnancial and economic\n",
      "forecasting. Summary statistics on series lengths are also listed, showing wide variability in length,\n",
      "as well as a characterization ( smooth vserratic ) that follows Syntetos et al. (2005), and is based\n",
      "on the squared coefﬁcient of variation of the series. All series have positive observed values at all\n",
      "time-steps; as such, none can be considered intermittent orlumpy per Syntetos et al. (2005).\n",
      "A.3 TOURISM DATASET DETAILS\n",
      "Table 4 outlines the composition of the TOURISM dataset across forecast horizons by listing the\n",
      "number of time series based on their frequency. Summary statistics on series lengths are listed,\n",
      "showing wide variability in length. All series have positive observed values at all time-steps. In\n",
      "contrast to M4 and M3 datasets, TOURISM includes a much higher fraction of erratic series.\n",
      "13' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 12}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "Table 3: Composition of the M3 dataset: the number of time series based on their sampling frequency\n",
      "and type.\n",
      "Frequency / Horizon\n",
      "Type Yearly/6 Quarterly/8 Monthly/18 Other/8 Total\n",
      "Demographic 245 57 111 0 413\n",
      "Finance 58 76 145 29 308\n",
      "Industry 102 83 334 0 519\n",
      "Macro 83 336 312 0 731\n",
      "Micro 146 204 474 4 828\n",
      "Other 11 0 52 141 204\n",
      "Total 645 756 1,428 174 3,003\n",
      "Min. Length 20 24 66 71\n",
      "Max. Length 47 72 144 104\n",
      "Mean Length 28.4 48.9 117.3 76.6\n",
      "SD Length 9.9 10.6 28.5 10.9\n",
      "% Smooth 90% 99% 98% 100%\n",
      "% Erratic 10% 1% 2% 0%\n",
      "Table 4: Composition of the TOURISM dataset: the number of time series based on their sampling\n",
      "frequency.\n",
      "Frequency / Horizon\n",
      "Yearly/4 Quarterly/8 Monthly/24 Total\n",
      "518 427 366 1,311\n",
      "Min. Length 11 30 91\n",
      "Max. Length 47 130 333\n",
      "Mean Length 24.4 99.6 298\n",
      "SD Length 5.5 20.3 55.7\n",
      "% Smooth 77% 61% 49%\n",
      "% Erratic 23% 39% 51%\n",
      "14' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 13}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "Table 5: sMAPE on the validation set, generic ar-\n",
      "chitecture. sMAPE for varying number of stacks,\n",
      "each having one residual block.\n",
      "Stacks s MAPE\n",
      "1 11.154\n",
      "3 11.061\n",
      "9 10.998\n",
      "18 10.950\n",
      "30 10.937Table 6: sMAPE on the validation set, inter-\n",
      "pretable architecture. Ablation of the synergy\n",
      "of the layers with different basis functions and\n",
      "multi-block stack gain.\n",
      "Detrend Seasonality s MAPE\n",
      "0 2 11.189\n",
      "2 0 11.572\n",
      "1 1 11.040\n",
      "3 3 10.986\n",
      "B A BLATION STUDIES\n",
      "B.1 L AYER STACKING AND BASIS SYNERGY\n",
      "We performed an ablation study on the validation set, using sMAPE metric as performance criterion.\n",
      "We addressed two speciﬁc questions with this study. First, Is stacking layers helpful? Second, Does\n",
      "the architecture based on the combination of layers with different basis functions results in better\n",
      "performance than the architecture using only one layer type?\n",
      "Layer stacking. We start our study with the generic architecture that consists of stacks of one\n",
      "residual block of 5 FC layers each of the form Fig. 1 and we increase the number of stacks. Results\n",
      "presented in Table 5 conﬁrm that increasing the number of stacks decreases error and at certain point\n",
      "the gain saturates. We would like to mention that the network having 30 stack of depth 5 is in fact a\n",
      "very deep network of total depth 150 layers.\n",
      "Basis synergy. Stacking works well for the interpretable architecture as can be seen in Table 6\n",
      "depicting the results of ablating the interpretable architecture conﬁguration. Here we experiment\n",
      "with the architecture that is composed of 2 stacks, stack one is trend model and stack two is the\n",
      "seasonality model. Each stack has variable number of residual blocks and each residual block has 5\n",
      "FC layers. We found that this architecture works best when all weights are shared within stack. We\n",
      "clearly see that increasing the number of layers improves performance. The largest network is 60\n",
      "layers deep. On top of that, we observe that the architecture that consists of stacks based on different\n",
      "basis functions wins over the architecture based on the same stack. It looks like chaining stacks of\n",
      "different nature results in synergistic effects. This is logical as function classes that can be modelled\n",
      "by trend and seasonality stacks have small overlap.\n",
      "B.2 E NSEMBLE SIZE\n",
      "Figure 3 demonstrates that increasing the ensemble size results in improved performance. Most\n",
      "importantly, according to Figure 3, N-BEATS achieves state-of-the-art performance even if compara-\n",
      "tively small ensemble size of 18 models is used. Therefore, computational efﬁciency of N-BEATS\n",
      "can be traded very effectively for performance and there is no over-reliance of the results on large\n",
      "ensemble size.\n",
      "B.3 D OUBLY RESIDUAL STACKING\n",
      "In Section 3.2 we described the proposed doubly residual stacking (DRESS) principle, which is the\n",
      "topological foundation of N-BEATS. The topology is based on both (i) running a residual backcast\n",
      "connection and (ii) producing partial block-level forecasts that are further aggregated at stack and\n",
      "model levels to produce the ﬁnal model-level forecast. In this section we conduct a study to conﬁrm\n",
      "the accuracy effectiveness of this topology compared to several alternatives. The methodology\n",
      "underlying this study is that we remove either the backcast or partial forecast links or both and track\n",
      "how this affects the forecasting metrics. We keep the number of parameters in the network for each\n",
      "of the architectural alternatives ﬁxed by using the same number of layers in the network (we used\n",
      "default hyperparameter settings reported in Table 18). The architectural alternatives are depicted in\n",
      "Figure 4 and described in detail below.\n",
      "15' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 14}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "18 36 90 180\n",
      "Ensemble Size0.7970.7980.7990.8000.8010.802OWA\n",
      "Figure 3: M4 test performance ( OWA ) as a function of ensemble size, based on N-BEATS-G. This\n",
      "ﬁgure shows that N-BEATS loses less than 0.5% in terms of OWA performance even if 10 times\n",
      "smaller ensemble size is used.\n",
      "N-BEATS-DRESS is depicted in Fig. 4a. This is the default conﬁguration of N-BEATS using doubly\n",
      "residual stacking described in Section 3.2.\n",
      "PARALLEL is depicted in Fig. 4b. This is the alternative where the backward residual connection is\n",
      "disabled and the overall model input is fed to every block. The blocks then forecast in parallel using\n",
      "the same input and their individual outputs are summed to make the ﬁnal forecast.\n",
      "NO-RESIDUAL is depicted in Fig. 4c. This is the alternative where the backward residual connection\n",
      "is disabled. Unlike PARALLEL, in this case the backcast forecast of the previous block is fed as input\n",
      "to the next block. Unlike the usual feed-forward network, in the NO-RESIDUAL architecture, each\n",
      "block makes a partial forecast and their individual outputs are summed to make the ﬁnal forecast.\n",
      "LAST-FORWARD is depicted in Fig. 4d. This is the alternative where the backward residual\n",
      "connection is active, however the model level forecast is derived only from the last block. So, the\n",
      "partial forward forecasts are disabled. This is the architecture that is closest to the classical residual\n",
      "network.\n",
      "NO-RESIDUAL-LAST-FORWARD is depicted in Fig. 4f. This is the alternative where both\n",
      "backward residual and the partial forward connections are disabled. This is therefore a simple\n",
      "feed-forward network, but very deep.\n",
      "The quantitative ablation study results on the M4 dataset are reported in Tables 7–10. N-BEATS-\n",
      "DRESS model is essentially N-BEATS model in this study. For this study we used ensemble size\n",
      "of 18. Since the ensemble size is 18 for N-BEATS-DRESS, as opposed to 180 used for N-BEATS,\n",
      "the OWA metric reported in Table 9 for N-BEATS-DRESS is higher than the OWA reported for\n",
      "N-BEATS-G in Table 12. Note that both results align well with OWA reported in Figure 3 for different\n",
      "ensemble sizes, as part of the ensemble size ablation conducted in Section B.2.\n",
      "The results presented in Tables 7–10 demonstrate that the doubly residual stacking topology provides\n",
      "a clear overall advantage over the alternative architectures in which either backcast residual links or\n",
      "the partial forward forecast links are disabled.\n",
      "16' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 15}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "Block 1\n",
      "Block 2\n",
      "Block  K–\n",
      "–\n",
      "–+Stack Input\n",
      "Stack\n",
      "forecastStack 1\n",
      "Stack 2\n",
      "Stack M+Global forecast\n",
      "(model output)Model Input\n",
      "Stack output\n",
      "(to next stack)\n",
      "(a) N-BEATS-DRESS\n",
      "Block 1\n",
      "Block 2\n",
      "Block  K+\n",
      "Stack output\n",
      "(to next stack)Stack Input\n",
      "Stack\n",
      "forecastStack 1\n",
      "Stack 2\n",
      "Stack M+Global forecast\n",
      "(model output)Model Input (b) PARALLEL\n",
      "Block 1\n",
      "Block 2\n",
      "Block  K+\n",
      "Stack residual\n",
      "(to next stack)Stack Input\n",
      "Stack\n",
      "forecastStack 1\n",
      "Stack 2\n",
      "Stack M+Global forecast\n",
      "(model output)Model Input (c) NO-RESIDUAL\n",
      "Block 1\n",
      "Block 2\n",
      "Block  K–\n",
      "–\n",
      "–\n",
      "Stack residual\n",
      "(to next stack)Stack Input\n",
      "Stack\n",
      "forecastStack 1\n",
      "Stack 2\n",
      "Stack MGlobal forecast\n",
      "(model output)Model Input\n",
      "(d) LAST-FORWARD\n",
      "Block 1\n",
      "Block 2\n",
      "Block  K\n",
      "Stack residual\n",
      "(to next stack)Stack Input\n",
      "Stack\n",
      "forecastStack 1\n",
      "Stack 2\n",
      "Stack MGlobal forecast\n",
      "(model output)Model Input (e) NO-RESIDUAL-LAST-\n",
      "FORWARD\n",
      "Block 1\n",
      "Block 2\n",
      "Block  K–\n",
      "–\n",
      "–+Stack Input\n",
      "Stack\n",
      "forecastStack 1\n",
      "Stack 2\n",
      "Stack M+Global forecast\n",
      "(model output)Model Input\n",
      "Stack output\n",
      "(to next stack)Model Input(f) RESIDUAL-INPUT\n",
      "Figure 4: The architectural conﬁgurations used in the ablation study of the doubly residual stack.\n",
      "Symbol⋄denotes unconnected output.\n",
      "Table 7: Performance on the M4 test set, sMAPE . Lower values are better. The results are obtained on\n",
      "the ensemble of 18 generic models.\n",
      "Yearly Quarterly Monthly Others Average\n",
      "(23k) (24k) (48k) (5k) (100k)\n",
      "PARALLEL-G 13.279 9.558 12.510 3.691 11.538\n",
      "NO-RESIDUAL-G 13.195 9.555 12.451 3.759 11.493\n",
      "LAST-FORWARD-G 13.200 9.322 12.352 3.703 11.387\n",
      "NO-RESIDUAL-LAST-FORWARD-G 15.386 11.346 15.282 6.673 13.931\n",
      "RESIDUAL-INPUT-G 13.264 9.545 12.316 3.692 11.438\n",
      "N-BEATS-DRESS-G 13.211 9.217 12.122 3.636 11.251\n",
      "Table 8: Performance on the M4 test set, sMAPE . Lower values are better. The results are obtained on\n",
      "the ensemble of 18 interpretable models.\n",
      "Yearly Quarterly Monthly Others Average\n",
      "(23k) (24k) (48k) (5k) (100k)\n",
      "PARALLEL-I 13.207 9.530 12.500 3.710 11.510\n",
      "NO-RESIDUAL-I 13.075 9.707 12.708 4.007 11.637\n",
      "LAST-FORWARD-I 13.168 9.547 12.111 3.599 11.313\n",
      "NO-RESIDUAL-LAST-FORWARD-I 13.067 10.207 15.177 4.912 12.986\n",
      "RESIDUAL-INPUT-I 13.104 9.716 12.814 4.005 11.697\n",
      "N-BEATS-DRESS-I 13.155 9.286 12.009 3.642 11.201\n",
      "17' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 16}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "Table 9: Performance on the M4 test set, OWA . Lower values are better. The results are obtained on\n",
      "the ensemble of 18 generic models.\n",
      "Yearly Quarterly Monthly Others Average\n",
      "(23k) (24k) (48k) (5k) (100k)\n",
      "PARALLEL-G 0.780 0.832 0.852 0.844 0.822\n",
      "NO-RESIDUAL-G 0.774 0.831 0.851 0.853 0.819\n",
      "LAST-FORWARD-G 0.774 0.808 0.840 0.846 0.811\n",
      "NO-RESIDUAL-LAST-FORWARD-G 0.948 1.029 1.095 1.296 1.030\n",
      "RESIDUAL-INPUT-G 0.779 0.831 0.840 0.844 0.817\n",
      "N-BEATS-DRESS-G 0.776 0.800 0.823 0.835 0.803\n",
      "Table 10: Performance on the M4 test set, OWA . Lower values are better. The results are obtained on\n",
      "the ensemble of 18 interpretable models.\n",
      "Yearly Quarterly Monthly Others Average\n",
      "(23k) (24k) (48k) (5k) (100k)\n",
      "PARALLEL-I 0.776 0.831 0.857 0.845 0.821\n",
      "NO-RESIDUAL-I 0.769 0.848 0.886 0.886 0.833\n",
      "LAST-FORWARD-I 0.773 0.836 0.825 0.817 0.808\n",
      "NO-RESIDUAL-LAST-FORWARD-I 0.771 0.900 1.085 1.016 0.922\n",
      "RESIDUAL-INPUT-I 0.771 0.848 0.892 0.887 0.836\n",
      "N-BEATS-DRESS-I 0.771 0.805 0.819 0.836 0.800\n",
      "18' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 17}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "Table 11: Performance on the M4 test set, s MAPE . Lower values are better. Red – second best.\n",
      "Yearly Quarterly Monthly Others Average\n",
      "(23k) (24k) (48k) (5k) (100k)\n",
      "Best pure ML 14.397 11.031 13.973 4.566 12.894\n",
      "Best statistical 13.366 10.155 13.002 4.682 11.986\n",
      "Best ML/TS combination 13.528 9.733 12.639 4.118 11.720\n",
      "DL/TS hybrid, M4 winner 13.176 9.679 12.126 4.014 11.374\n",
      "N-BEATS-G 13.023 9.212 12.048 3.574 11.168\n",
      "N-BEATS-I 12.924 9.287 12.059 3.684 11.174\n",
      "N-BEATS-I+G 12.913 9.213 12.024 3.643 11.135\n",
      "Table 12: Performance on the M4 test set, OWA and M4 rank. Lower values are better. Red – second\n",
      "best.\n",
      "Yearly Quarterly Monthly Others Average Rank\n",
      "(23k) (24k) (48k) (5k) (100k)\n",
      "Best pure ML 0.859 0.939 0.941 0.991 0.915 23\n",
      "Best statistical 0.788 0.898 0.905 0.989 0.861 8\n",
      "Best ML/TS combination 0.799 0.847 0.858 0.914 0.838 2\n",
      "DL/TS hybrid, M4 winner 0.778 0.847 0.836 0.920 0.821 1\n",
      "N-BEATS-G 0.765 0.800 0.820 0.822 0.797\n",
      "N-BEATS-I 0.758 0.807 0.824 0.849 0.798\n",
      "N-BEATS-I+G 0.758 0.800 0.819 0.840 0.795\n",
      "C D ETAILED EMPIRICAL RESULTS\n",
      "C.1 D ETAILED RESULTS : M4 D ATASET\n",
      "Tables 11 and 12 present our key quantitative empirical results showing that the proposed model\n",
      "achieves the state of the art performance on the challenging M4 benchmark. We study the performance\n",
      "of two model conﬁgurations: generic (Ours-G) and interpretable (Ours-I), as well as Ours-I+G\n",
      "(ensemble of all models from Ours-G and Ours-I). We compare against 4 representatives from the\n",
      "M4 competition: each best in their respective model class. Best pure ML is the submission by B.\n",
      "Trotta, the best entry among the 6 pure ML models. Best statistical is the best pure statistical model\n",
      "by N.Z. Legaki and K. Koutsouri. Best ML/TS combination is the model by P. Montero-Manso, T.\n",
      "Talagala, R.J. Hyndman and G. Athanasopoulos, second best entry, gradient boosted tree over a few\n",
      "statistical time series models. Finally, DL/TS hybrid is the winner of M4 competition (Smyl, 2020).\n",
      "N-BEATS outperforms all other approaches on all the studied subsets of time series. The average\n",
      "OWA gap between our generic model and the M4 winner ( 0.821−0.795=0.026) is greater than the\n",
      "gap between the M4 winner and the second entry (0 .838−0.821=0.017).\n",
      "A more granular and detailed statistical analysis of our results on M4 is provided in Table 13. This\n",
      "table ﬁrst presents the sMAPE for N-BEATS, decomposed by M4 time series sub-type and sampling\n",
      "frequency (upper part). Then (lower part), it shows the average sMAPE difference between the\n",
      "N-BEATS results and the M4 winner (TS/DL hybrid by S. Smyl), adding the standard error of that\n",
      "difference (in parentheses); bold entries indicate statistical signiﬁcance at the 99% level based on a\n",
      "two-sided paired t-test.\n",
      "We note that each cross-section of the M4 dataset into horizon and type may be regarded as an\n",
      "independent mini-dataset. We observe that over those mini-datasets there is a preponderance of\n",
      "statistically signiﬁcant differences between N-BEATS and Smyl (18 cases out of 31) to the advantage\n",
      "of N-BEATS. This provides evidence that (i) the improvement observed on average in Tables 11\n",
      "and 12 is statistically signiﬁcant and consistent over smaller subsets of M4 and (ii) N-BEATS\n",
      "generalizes well over time series of different types and sampling frequencies.\n",
      "19' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 18}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "Table 13: Performance decomposition on non-overlapping subsets of the M4 test set and comparison\n",
      "with the Smyl model results.\n",
      "Demographic Finance Industry Macro Micro Other\n",
      "sMAPE per M4 series type and sampling frequency\n",
      "Yearly 8 .931 13 .741 16 .317 13 .327 10 .489 13 .320\n",
      "Quarterly 9 .219 10 .787 8 .628 8 .576 9 .264 6 .250\n",
      "Monthly 4 .357 13 .353 12 .657 12 .571 13 .627 11 .595\n",
      "Weekly 4 .580 3 .004 9 .258 7 .220 10 .425 6 .183\n",
      "Daily 6 .351 3 .467 3 .835 2 .525 2 .299 2 .885\n",
      "Hourly 8 .197\n",
      "Average sMAPE difference vs Smyl model , computed as N-BEATS – Smyl.\n",
      "Standard error of the mean displayed in parenthesis.\n",
      "Bold entries are signiﬁcant at the 99% level (2-sided paired t-test).\n",
      "Yearly−0.749−0.337−0.065−0.386−0.168−0.157\n",
      "(0.119) ( 0.065) ( 0.087) ( 0.085) ( 0.056) ( 0.140)\n",
      "Quarterly−0.651−0.281−0.328−0.712−0.523−0.029\n",
      "(0.085) ( 0.047) ( 0.043) ( 0.060) ( 0.051) ( 0.083)\n",
      "Monthly−0.185−0.379−0.419 0.089 0.338−0.279\n",
      "(0.023) ( 0.034) ( 0.036) ( 0.039) ( 0.034) ( 0.162)\n",
      "Weekly−0.336−1.075−0.937−1.627−3.029−1.193\n",
      "(0.270) ( 0.221) ( 1.399) ( 0.770) ( 0.378) ( 0.772)\n",
      "Daily 0 .191−0.098−0.124−0.026−0.367−0.037\n",
      "(0.231) ( 0.018) ( 0.025) ( 0.057) ( 0.013) ( 0.015)\n",
      "Hourly −1.132\n",
      "(0.163)\n",
      "20' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 19}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "Table 14: Performance on the M3 test set, Average sMAPE , aggregate over all forecast horizons\n",
      "(Yearly: 1-6, Quarterly: 1-8, Monthly: 1-18, Other: 1-8, Average: 1-18). Lower values are better.\n",
      "Red – second best.†Numbers are computed by us.\n",
      "Yearly Quarterly Monthly Others Average\n",
      "(645) (756) (1428) (174) (3003)\n",
      "Naïve2 17.88 9.95 16.91 6.30 15.47\n",
      "ARIMA (B–J automatic) 17.73 10.26 14.81 5.06 14.01\n",
      "Comb S-H-D 17.07 9.22 14.48 4.56 13.52\n",
      "ForecastPro 17.14 9.77 13.86 4.60 13.19\n",
      "Theta 16.90 8.96 13.85 4.41 13.01\n",
      "DOTM (Fiorucci et al., 2016) 15.94 9.28 13.74 4.58 12.90\n",
      "EXP (Spiliotis et al., 2019) 16.39 8.98 13.43 5.46 12 .71†\n",
      "LGT (Smyl & Kuber, 2016) 15.23 n/a n/a 4.26 n/a\n",
      "BaggedETS.BC (Bergmeir et al., 2016) 17.49 9.89 13.74 n/a n/a\n",
      "N-BEATS-G 16.2 8.92 13.19 4.19 12.47\n",
      "N-BEATS-I 15.84 9.03 13.15 4.30 12.43\n",
      "N-BEATS-I+G 15.93 8.84 13.11 4.24 12.37\n",
      "C.2 D ETAILED RESULTS : M3 D ATASET\n",
      "Results for M3 dataset are provided in Table 14. The performance metric is calculated using the\n",
      "earlier version of s MAPE , deﬁned speciﬁcally for the M3 competition:1\n",
      "sMAPE =200\n",
      "HH\n",
      "∑\n",
      "i=1|yT+i−ˆyT+i|\n",
      "yT+i+ˆyT+i. (4)\n",
      "For some of the methods, either average sMAPE was not reported or sMAPE for some of the splits was\n",
      "not reported in their respective publications. Below, we list those cases. BaggedETS.BC (Bergmeir\n",
      "et al., 2016) has not reported numbers on Others. LGT (Smyl & Kuber, 2016) did not report results on\n",
      "Monthly and Quarterly data. According to the authors, the underlying RNN had problems dealing with\n",
      "raw seasonal data, the ETS based pre-processing was not effective and the LGT pre-processing was\n",
      "not computationally feasible given comparatively large number of time series and their comparatively\n",
      "large length (Smyl & Kuber, 2016). Finally, EXP (Spiliotis et al., 2019) reported average performance\n",
      "computed using a different methodology than the default M3 and M4 methodology (source: personal\n",
      "communication with the authors). For the latter method we recomputed the Average sMAPE based on\n",
      "the previously reported Yearly, Quarterly and Monthly splits. To calculate it, we follow the M3, M4\n",
      "and TOURISM competition methodology and compute the average metric as the average over all time\n",
      "series and over all forecast horizons. Given the performance metric values aggregated over Yearly,\n",
      "Quarterly and Monthly splits, the average can be computed straightforwardly as:\n",
      "sMAPE Average =NYear\n",
      "NTotsMAPE Year+NQuart\n",
      "NTotsMAPE Quart+NMonth\n",
      "NTotsMAPE Month+NOthers\n",
      "NTotsMAPE Others.\n",
      "(5)\n",
      "Here NTot=NYear+NQuart+NMonth+NOthers andNYear=6×645,NQuart=8×756,NMonth =18×\n",
      "1428,NOthers =8×174. It is clear that for each split, its Nis the product of its respective number of\n",
      "time series and its largest forecast horizon.\n",
      "1With minor differences compared to the sMAPE deﬁnition used for M4. Please refer to Appendix A\n",
      "in (Makridakis & Hibon, 2000) for the mathematical deﬁnition.\n",
      "21' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 20}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "Table 15: Performance on the TOURISM test set, Average MAPE , aggregate over all forecast horizons\n",
      "(Yearly: 1-4, Quarterly: 1-8, Monthly: 1-24, Average: 1-24). Lower values are better. Red – second\n",
      "best.\n",
      "Yearly Quarterly Monthly Average\n",
      "(518) (427) (366) (1311)\n",
      "Statistical benchmarks (Athanasopoulos et al., 2011)\n",
      "SNaïve 23.61 16.46 22.56 21.25\n",
      "Theta 23.45 16.15 22.11 20.88\n",
      "ForePro 26.36 15.72 19.91 19.84\n",
      "ETS 27.68 16.05 21.15 20.88\n",
      "Damped 28.15 15.56 23.47 22.26\n",
      "ARIMA 28.03 16.23 21.13 20.96\n",
      "Kaggle competitors (Athanasopoulos & Hyndman, 2011)\n",
      "SaliMali n/a 14.83 19.64 n/a\n",
      "LeeCBaker 22.73 15.14 20.19 19.35\n",
      "Stratometrics 23.15 15.14 20.37 19.52\n",
      "Robert n/a 14.96 20.28 n/a\n",
      "Idalgo n/a 15.07 20.55 n/a\n",
      "N-BEATS-G (Ours) 21.67 14.71 19.17 18.47\n",
      "N-BEATS-I (Ours) 21.55 15.22 19.82 18.97\n",
      "N-BEATS-I+G (Ours) 21.44 14.78 19.29 18.52\n",
      "C.3 D ETAILED RESULTS :TOURISM DATASET\n",
      "Detailed results for the TOURISM competition dataset are provided in Table 15. The respective Kaggle\n",
      "competition was divided into two parts: (i) Yearly time series forecasting and (ii) Quarterly/Monthly\n",
      "time series forecasting (Athanasopoulos & Hyndman, 2011). Some of the participants chose to\n",
      "take part only in the second part. Therefore, In addition to entries present in Table 1, we report\n",
      "competitors from (Athanasopoulos & Hyndman, 2011) that have missing results in Yearly compe-\n",
      "tition. In particular, SaliMali team is the winner of the Quarterly/Monthly time series forecasting\n",
      "competition (Brierley, 2011). Their approach is based on a weighted ensemble of statistical methods.\n",
      "Teams Robert andIdalgo used unknown approaches. We can see from Table 15 that N-BEATS\n",
      "achieves state-of-the-art performance on all subsets of TOURISM dataset. On average, it is state of the\n",
      "art and it gains 4.2% over the best-known approach LeeCBaker , and 11.5% over auto-ARIMA.\n",
      "The average metrics have not been reported in the original competition results (Athanasopoulos et al.,\n",
      "2011; Athanasopoulos & Hyndman, 2011). Therefore, in Table 15, we present the Average MAPE\n",
      "metric calculated by us based on the previously reported Yearly, Quarterly and Monthly splits. To\n",
      "calculate it, we follow the M4 competition methodology and compute the average metric as the\n",
      "average over all time series and over all forecast horizons. Given the performance metric values\n",
      "aggregated over Yearly, Quarterly and Monthly splits, the average can be computed straightforwardly\n",
      "as:\n",
      "MAPE Average =NYear\n",
      "NTotMAPE Year+NQuart\n",
      "NTotMAPE Quart+NMonth\n",
      "NTotMAPE Month. (6)\n",
      "Here NTot=NYear+NQuart+NMonth andNYear=4×518,NQuart=8×427,NMonth =24×366. It is\n",
      "clear that for each split, its Nis the product of its respective number of time series and its largest\n",
      "forecast horizon.\n",
      "22' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 21}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "C.4 D ETAILED RESULTS :ELECTRICITY AND TRAFFIC DATASETS\n",
      "In this experiment we are comparing the performances of MatFact (Yu et al., 2016), DeepAR (Flunkert\n",
      "et al., 2017) (Amazon Labs), Deep State (Rangapuram et al., 2018a) (Amazon Labs), Deep Fac-\n",
      "tors (Wang et al., 2019) (Amazon Labs), and N-BEATS models on ELECTRICITY2(Dua & Graff,\n",
      "2017) and TRAFFIC3(Dua & Graff, 2017) datasets. The results are presented in in Table 16.\n",
      "Both datasets are aggregated to hourly data, but using different aggregation operations: sum for\n",
      "ELECTRICITY and mean for TRAFFIC . The hourly aggregation is done so that all the points available\n",
      "in(h−1 : 00,h: 00]hours are aggregated to hour h, thus if original dataset starts on 2011-01-01\n",
      "00:15 then the ﬁrst time point after aggregation will be 2011-01-01 01:00. For the ELECTRICITY\n",
      "dataset we removed the ﬁrst year from training set, to match the training set used in (Yu et al., 2016),\n",
      "based on the aggregated dataset downloaded from, presumable authors’, github repository4. We also\n",
      "made sure that data points for both ELECTRICITY and TRAFFIC datasets after aggregation match\n",
      "those used in (Yu et al., 2016). The authors of MatFact model were using the last 7 days of datasets\n",
      "as test set, but papers from Amazon are using different splits, where the split points are provided by a\n",
      "date. Changing split points without a well grounded reason adds uncertainties to the comparability of\n",
      "the models performances and creates challenges to the reproducibility of the results, thus we were\n",
      "trying to match all different splits in our experiments. It was especially challenging on TRAFFIC\n",
      "dataset, where we had to use some heuristics to ﬁnd records dates; the dataset authors state: “ The\n",
      "measurements cover the period from Jan. 1st 2008 to Mar. 30th 2009” and “ We remove public\n",
      "holidays from the dataset, as well as two days with anomalies (March 8th 2009 and March 9th 2008)\n",
      "where all sensors were muted between 2:00 and 3:00 AM. ” , but we failed to match a part of the\n",
      "provided labels of week days to actual dates. Therefore, we had to assume that the actual list of gaps,\n",
      "which include holidays and anomalous days, is the following:\n",
      "1. Jan. 1, 2008 (New Year’s Day)\n",
      "2. Jan. 21, 2008 (Martin Luther King Jr. Day)\n",
      "3. Feb. 18, 2008 (Washington’s Birthday)\n",
      "4. Mar. 9, 2008 (Anomaly day)\n",
      "5. May 26, 2008 (Memorial Day)\n",
      "6. Jul. 4, 2008 (Independence Day)\n",
      "7. Sep. 1, 2008 (Labor Day)\n",
      "8. Oct. 13, 2008 (Columbus Day)\n",
      "9. Nov. 11, 2008 (Veterans Day)\n",
      "10. Nov. 27, 2008 (Thanksgiving)\n",
      "11. Dec. 25, 2008 (Christmas Day)\n",
      "12. Jan. 1, 2009 (New Year’s Day)\n",
      "13. Jan. 19, 2009 (Martin Luther King Jr. Day)\n",
      "14. Feb. 16, 2009 (Washington’s Birthday)\n",
      "15. Mar. 8, 2009 (Anomaly day)\n",
      "The ﬁrst 6 gaps were conﬁrmed by the gaps in labels, but the rest were more than 1 day apart from any\n",
      "public holiday of years 2008 and 2009 in San Francisco, California and US. More over the number of\n",
      "gaps we found in the labels provided by dataset authors is 10, while the number of days between Jan.\n",
      "1st 2008 and Mar. 30th 2009 is 455, assuming that Jan. 1st 2008 was skipped from the values and\n",
      "labels we should end up with either 454 −10=444 instead of 440 days or different end date.\n",
      "The metric is reported in Normalized deviation (ND) as in (Yu et al., 2016) which is equal to p50\n",
      "loss used in DeepAR, Deep State, and Deep Factors papers.\n",
      "2https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014\n",
      "3https://archive.ics.uci.edu/ml/datasets/PEMS-SF\n",
      "4https://github.com/rofuyu/exp-trmf-nips16/blob/master/python/exp-scripts/datasets/download-data.sh\n",
      "23' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 22}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "ND=∑i,t|ˆYit−Yit|\n",
      "∑i,t|Yit|(7)\n",
      "Table 16: ND Performance on the ELECTRICITY and TRAFFIC test sets.\n",
      "1Split used in DeepAR (Flunkert et al., 2017) and Deep State (Rangapuram et al., 2018a).\n",
      "2Split used in Deep Factors (Wang et al., 2019).\n",
      "†Numbers reported by (Flunkert et al., 2017), which are different from the original MatFact paper,\n",
      "hypothetically due to changed split point.\n",
      "ELECTRICITY TRAFFIC\n",
      "2014-09-0112014-03-312last 7 days 2008-06-1512008-01-142last 7 days\n",
      "MatFact 0.16†n/a 0.255 0.20†n/a 0.187\n",
      "DeepAR 0.07 0.272 n/a 0.17 0.296 n/a\n",
      "Deep State 0.083 n/a n/a 0.167 n/a n/a\n",
      "Deep Factors n/a 0.112 n/a n/a 0.225 n/a\n",
      "N-BEATS-G (ours) 0.064 0.065 0.171 0.114 0.230 0.112\n",
      "N-BEATS-I (ours) 0.073 0.072 0.185 0.114 0.231 0.110\n",
      "N-BEATS-I+G (ours) 0.067 0.067 0.178 0.114 0.230 0.111\n",
      "Contrary to Amazon models N-BEATS does not use any covariates, like day-of-week, hour-of-day,\n",
      "etc.\n",
      "The N-BEATS architecture used in this experiment is exactly the same as used in M4, M3 and\n",
      "TOURISM datasets, the only difference is history size and the number of iterations. These parameters\n",
      "were chosen based on performance on validation set. Where the validation set consists of 7 consecutive\n",
      "days right before the test set. After the parameters are chosen the model is retrained on training set\n",
      "which includes the validation set, then tested on test set. The model is trained once and tested on test\n",
      "set using rolling window operation described in (Yu et al., 2016).\n",
      "24' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 23}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "C.5 D ETAILED RESULTS :COMPARE TO DEEPAR, D EEPSTATE SPACE MODELS\n",
      "Table 17 compares ND (7) performance of DeepAR, DeepState models published in (Rangapuram\n",
      "et al., 2018a) and N-BEATS.\n",
      "Table 17: ND Performance of DeepAR, Deep State Space, and N-BEATS models on M4-Hourly and\n",
      "TOURISM datasets\n",
      "M4 (Hourly) TOURISM (Monthly) TOURISM (Quarterly)\n",
      "DeepAR 0.09 0.107 0.11\n",
      "DeepState 0.044 0.138 0.098\n",
      "N-BEATS-G (ours) 0.023 0.097 0.080\n",
      "N-BEATS-I (ours) 0.027 0.103 0.079\n",
      "N-BEATS-I+G (ours) 0.025 0.099 0.077\n",
      "25' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 24}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "Table 18: Settings of hyperparameters across subsets of M4, M3, TOURISM datasets.\n",
      "M4 M3 TOURISM\n",
      "Yly Qly Mly Wly Dly Hly Yly Qly Mly Other Yly Qly Mly\n",
      "Parameter N-BEATS-I\n",
      "LH 1.5 1.5 1.5 10 10 10 20 5 5 20 20 10 20\n",
      "Iterations 15K 15K 15K 5K 5K 5K 50 6K 6K 250 30 500 300\n",
      "Losses s MAPE /MAPE /MASE sMAPE /MAPE /MASE MAPE\n",
      "S-width 2048\n",
      "S-blocks 3\n",
      "S-block-layers 4\n",
      "T-width 256\n",
      "T-degree 2\n",
      "T-blocks 3\n",
      "T-block-layers 4\n",
      "Sharing STACK LEVEL\n",
      "Lookback period 2 H,3H,4H,5H,6H,7H\n",
      "Batch 1024\n",
      "Parameter N-BEATS-G\n",
      "LH 1.5 1.5 1.5 10 10 10 20 20 20 10 5 10 20\n",
      "Iterations 15K 15K 15K 5K 5K 5K 20 250 10K 250 30 100 100\n",
      "Losses s MAPE /MAPE /MASE sMAPE /MAPE /MASE MAPE\n",
      "Width 512\n",
      "Blocks 1\n",
      "Block-layers 4\n",
      "Stacks 30\n",
      "Sharing NO\n",
      "Lookback period 2 H,3H,4H,5H,6H,7H\n",
      "Batch 1024\n",
      "D H YPER -PARAMETER SETTINGS\n",
      "Table 18 presents the hyperparameter settings used to train models on different subsets of M4, M3\n",
      "and TOURISM datasets. A brief discussion of ﬁeld names in the table is warranted.\n",
      "Subset names Yly, Qly, Mly, Wly, Dly, Hly, Other correspond to yearly, quarterly, monthly, weekly,\n",
      "daily, hourly and other frequency subsets deﬁned in the original datasets.\n",
      "N-BEATS-I andN-BEATS-G correspond to the interpretable and generic model conﬁgurations\n",
      "deﬁned in Section 3.3.\n",
      "D.1 C OMMON PARAMETERS\n",
      "LHis the coefﬁcient deﬁning the length of training history immediately preceding the last point in\n",
      "the train part of the TS that is used to generate training samples. For example, if for M4 Yearly the\n",
      "forecast horizon is 6 and LHis 1.5, then we consider 1.5·6=9most recent points in the train dataset\n",
      "for each time series to generate training samples. A training sample from a given TS in M4 Yearly is\n",
      "then generated by choosing one of the most recent 9 points as an anchor. All the points preceding the\n",
      "anchor are used to create the input to N-BEATS, while the points following and including the anchor\n",
      "become training target. Target and history points that fall outside of the time series limits given the\n",
      "anchor position are ﬁlled with zeros and masked during the training. We observed that for subsets\n",
      "with large number of time series LHtends to be smaller and for subsets with smaller number of time\n",
      "series it tends to be larger. For example, in massive Yearly, Monthly, Quarterly subsets of M4 LHis\n",
      "equal to 1 .5; and in moderate to small Weekly, Daily, Hourly subsets of M4 LHis equal to 10.\n",
      "Iterations is the number of batches used to train N-BEATS.\n",
      "26' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 25}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "Losses is the set of loss functions that is used to build ensemble. We observed on the respective\n",
      "validation sets that for M4 and M3 mixing models trained on a variety of metrics resulted in\n",
      "performance gain. In the case of TOURISM dataset training only on MAPE led to the best validation\n",
      "scores.\n",
      "Sharing deﬁnes whether the coefﬁcients in the fully-connected layers are shared. We observed that\n",
      "the interpretable model works best when weights are shared across stack, while generic model works\n",
      "best when none of the weights are shared.\n",
      "Lookback period is the length of the history window forming the input to the model (please refer to\n",
      "Figure 1). This is the function of the forecast horizon length, H. In our experiments we mixed models\n",
      "with lookback periods 2H,3H,4H,5H,6H,7Hin one ensemble. As an example, for a forecast\n",
      "horizon length H=8and a lookback period 7H, the model’s input will consist of the history window\n",
      "of 7·8=56 samples.\n",
      "Batch is the batch size. We used batch size of 1024. We observed that the training was faster with\n",
      "larger batch sizes, however in our setup little gain was observed with batch sizes beyond 1024.\n",
      "D.2 N-BEATS-I PARAMETERS\n",
      "S-width is the width of the fully connected layers in the blocks comprising the seasonality stack of\n",
      "the interpretable model (please refer to Figure 1).\n",
      "S-blocks is the number of blocks comprising the seasonality stack of the interpretable model (please\n",
      "refer to Figure 1).\n",
      "S-block-layers is the number of fully-connected layers comprising one block in the seasonality\n",
      "stack of the interpretable model (preceding the ﬁnal fully-connected projection layers forming the\n",
      "backcast/forecast fork, please refer to Figure 1).\n",
      "T-width is the width of the fully connected layers in the blocks comprising the trend stack of the\n",
      "interpretable model (please refer to Figure 1).\n",
      "T-degree is the degree pof polynomial in the trend stack of the interpretable model (please refer to\n",
      "equation (2)).\n",
      "T-blocks is the number of blocks comprising the trend stack of the interpretable model (please refer\n",
      "to Figure 1).\n",
      "T-block-layers is the number of fully-connected layers comprising one block in the trend stack\n",
      "of the interpretable model (preceding the ﬁnal fully-connected projection layers forming the back-\n",
      "cast/forecast fork, please refer to Figure 1).\n",
      "D.3 N-BEATS-G PARAMETERS\n",
      "Width is the width of the fully connected layers in the blocks comprising the stacks of the generic\n",
      "model (please refer to Figure 1).\n",
      "Blocks is the number of blocks comprising the stack of the generic model (please refer to Figure 1).\n",
      "Block-layers is the number of fully-connected layers comprising one block in the stack of the generic\n",
      "model (preceding the ﬁnal fully-connected projection layers forming the backcast/forecast fork,\n",
      "please refer to Figure 1).\n",
      "27' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 26}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "0 1 2 3 4 5\n",
      "t0.80.91.0\n",
      "ACTUAL\n",
      "FORECAST-I\n",
      "FORECAST-G\n",
      "0 1 2 3 4 5\n",
      "t0.800.850.90STACK1-G\n",
      "0 1 2 3 4 5\n",
      "t0.0250.0500.075STACK2-G\n",
      "0 1 2 3 4 5\n",
      "t0.800.850.900.95\n",
      "STACK1-I\n",
      "0 1 2 3 4 5\n",
      "t0.020.030.040.05STACK2-I\n",
      "0 2 4 6\n",
      "t0.850.900.951.00\n",
      "ACTUAL\n",
      "FORECAST-I\n",
      "FORECAST-G\n",
      "0 2 4 6\n",
      "t0.860.880.90STACK1-G\n",
      "0 2 4 6\n",
      "t0.025\n",
      "0.0000.0250.050\n",
      "STACK2-G\n",
      "0 2 4 6\n",
      "t0.880.890.90STACK1-I\n",
      "0 2 4 6\n",
      "t0.05\n",
      "0.000.05STACK2-I\n",
      "0 5 10 15\n",
      "t0.40.60.81.0\n",
      "ACTUAL\n",
      "FORECAST-I\n",
      "FORECAST-G\n",
      "0 5 10 15\n",
      "t0.80.9 STACK1-G\n",
      "0 5 10 15\n",
      "t0.1\n",
      "0.0\n",
      "STACK2-G\n",
      "0 5 10 15\n",
      "t0.850.90STACK1-I\n",
      "0 5 10 15\n",
      "t0.3\n",
      "0.2\n",
      "0.1\n",
      "0.0STACK2-I\n",
      "0 2 4 6 8 10 12\n",
      "t0.60.81.0\n",
      "ACTUAL\n",
      "FORECAST-I\n",
      "FORECAST-G\n",
      "0 2 4 6 8 10 12\n",
      "t0.650.700.750.80\n",
      "STACK1-G\n",
      "0 2 4 6 8 10 12\n",
      "t0.0000.0250.0500.075\n",
      "STACK2-G\n",
      "0 2 4 6 8 10 12\n",
      "t0.650.700.750.80STACK1-I\n",
      "0 2 4 6 8 10 12\n",
      "t0.000.020.04STACK2-I\n",
      "0.0 2.5 5.0 7.5 10.0 12.5\n",
      "t0.960.981.00\n",
      "ACTUAL\n",
      "FORECAST-I\n",
      "FORECAST-G\n",
      "0.0 2.5 5.0 7.5 10.0 12.5\n",
      "t0.9740.976STACK1-G\n",
      "0.0 2.5 5.0 7.5 10.0 12.5\n",
      "t0.002\n",
      "0.001\n",
      "STACK2-G\n",
      "0.0 2.5 5.0 7.5 10.0 12.5\n",
      "t0.9740.976STACK1-I\n",
      "0.0 2.5 5.0 7.5 10.0 12.5\n",
      "t0.0003\n",
      "0.0002\n",
      "0.0001\n",
      "STACK2-I\n",
      "0 10 20 30 40\n",
      "t0.250.500.751.00\n",
      "ACTUAL\n",
      "FORECAST-I\n",
      "FORECAST-G\n",
      "(a) Combined\n",
      "0 10 20 30 40\n",
      "t0.20.40.6\n",
      "STACK1-G (b) Stack1-G\n",
      "0 10 20 30 40\n",
      "t0.02\n",
      "0.00\n",
      "STACK2-G (c) Stack2-G\n",
      "0 10 20 30 40\n",
      "t0.360.380.40\n",
      "STACK1-I (d) StackT-I\n",
      "0 10 20 30 40\n",
      "t0.2\n",
      "0.00.2\n",
      "STACK2-I (e) StackS-I\n",
      "Figure 5: The outputs of generic and the interpretable conﬁgurations, M4 dataset. Each row is one\n",
      "time series example per data frequency, top to bottom (Yearly: id Y3974, Quarterly: id Q11588,\n",
      "Monthly: id M19006, Weekly: id W246, Daily: id D404, Hourly: id H344). The magnitudes in a row\n",
      "are normalized by the maximal value of the actual time series for convenience. Column (a) shows the\n",
      "actual values (ACTUAL), the generic model forecast (FORECAST-G) and the interpretable model\n",
      "forecast (FORECAST-I). Columns (b) and (c) show the outputs of stacks 1 and 2 of the generic model,\n",
      "respectively; FORECAST-G is their summation. Columns (d) and (e) show the output of the Trend\n",
      "and the Seasonality stacks of the interpretable model, respectively; FORECAST-I is their summation.\n",
      "E D ETAILED SIGNAL TRACES OF INTERPRETABLE INPUTS PRESENTED IN\n",
      "FIGURE 2\n",
      "The goal of this section is to show the detailed traces (numeric values) of signals visualized in Fig. 2.\n",
      "This is to demonstrate that even though the StackT-I (Fig. 2 (d)) and StackS-I (Fig. 2 (e)) provide\n",
      "response lines different from the counterparts in Stack1-G (Fig. 2 (b)) and Stack2-G (Fig. 2 (c)), the\n",
      "summations in the combined line (Fig. 2 (a)) can still be very similar.\n",
      "First, we reproduce Fig. 5 for the convenience of the reader. Second, for each row in the ﬁgure, we\n",
      "produce a table showing the numeric values of each signal depicted in corresponding plots (please\n",
      "refer to Tables 19– 24). We make sure that the names of signals in ﬁgure legends and in the table\n",
      "columns match, such that they can easily be cross-referenced. It can be clearly seen in Tables 19– 24\n",
      "that (i) traces STACK1-I and STACK2-I sum up to trace FORECAST-I, (ii) traces STACK1-G and\n",
      "STACK2-G sum up to trace FORECAST-G, (iii) traces FORECAST-I and FORECAST-G are overall\n",
      "very similar even though their components may signiﬁcantly differ from each other.\n",
      "28' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 27}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "Table 19: Detailed traces of signals depicted in row 1 of Fig. 5, corresponding to the time series\n",
      "Yearly: id Y3974.\n",
      "ACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\n",
      "t\n",
      "0 0.780182 0.802068 0.806608 0.781290 0.020778 0.801294 0.005314\n",
      "1 0.802337 0.829223 0.841406 0.798422 0.030801 0.825271 0.016135\n",
      "2 0.840317 0.863683 0.883136 0.820196 0.043487 0.853114 0.030022\n",
      "3 0.889376 0.905962 0.929258 0.850250 0.055712 0.880833 0.048425\n",
      "4 0.930521 0.947028 0.967846 0.892221 0.054807 0.904393 0.063453\n",
      "5 0.976414 0.982307 1.000000 0.949748 0.032559 0.921360 0.078640\n",
      "Table 20: Detailed traces of signals depicted in row 2 of Fig. 5, corresponding to the time series\n",
      "Quarterly: id Q11588.\n",
      "ACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\n",
      "t\n",
      "0 0.830068 0.835964 0.829417 0.880435 -0.044471 0.852018 -0.022601\n",
      "1 0.927155 0.898949 0.891168 0.881626 0.017324 0.880124 0.011044\n",
      "2 0.979204 0.957379 0.948799 0.882549 0.074831 0.907149 0.041650\n",
      "3 0.857250 0.900612 0.891967 0.883830 0.016782 0.877959 0.014008\n",
      "4 0.895082 0.857230 0.847029 0.886096 -0.028866 0.852232 -0.005204\n",
      "5 0.981590 0.923832 0.911001 0.889972 0.033860 0.881140 0.029861\n",
      "6 1.000000 0.978128 0.965236 0.896085 0.082043 0.907475 0.057761\n",
      "7 0.910528 0.920632 0.915460 0.905062 0.015571 0.886941 0.028519\n",
      "Table 21: Detailed traces of signals depicted in row 3 of Fig. 5, corresponding to the time series\n",
      "Monthly: id M19006.\n",
      "ACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\n",
      "t\n",
      "0 1.000000 0.923394 0.928279 0.944660 -0.021266 0.922835 0.005444\n",
      "1 0.865248 0.822588 0.829924 0.937575 -0.114987 0.867619 -0.037695\n",
      "2 0.638298 0.693820 0.717119 0.930295 -0.236475 0.810818 -0.093699\n",
      "3 0.531915 0.594375 0.612377 0.922890 -0.328515 0.757199 -0.144823\n",
      "4 0.468085 0.579403 0.595221 0.915428 -0.336025 0.747151 -0.151930\n",
      "5 0.539007 0.602615 0.620809 0.907977 -0.305362 0.755078 -0.134269\n",
      "6 0.581560 0.653387 0.682669 0.900606 -0.247219 0.774561 -0.091891\n",
      "7 0.666667 0.747440 0.765814 0.893385 -0.145945 0.799594 -0.033781\n",
      "8 0.737589 0.817883 0.835577 0.886382 -0.068498 0.817218 0.018359\n",
      "9 0.765957 0.862568 0.856962 0.879665 -0.017097 0.822099 0.034862\n",
      "10 0.851064 0.873448 0.880074 0.873304 0.000145 0.833473 0.046601\n",
      "11 0.893617 0.878186 0.871103 0.867367 0.010819 0.829537 0.041566\n",
      "12 0.858156 0.834448 0.853549 0.861923 -0.027475 0.816527 0.037022\n",
      "13 0.695035 0.785341 0.776687 0.857040 -0.071699 0.782536 -0.005850\n",
      "14 0.446809 0.662443 0.697788 0.852789 -0.190345 0.745623 -0.047835\n",
      "15 0.382979 0.623196 0.624614 0.849236 -0.226040 0.711553 -0.086939\n",
      "16 0.453901 0.598511 0.625150 0.846451 -0.247941 0.712130 -0.086980\n",
      "17 0.539007 0.668231 0.652175 0.844504 -0.176272 0.716925 -0.064750\n",
      "29' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 28}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "Table 22: Detailed traces of signals depicted in row 4 of Fig. 5, corresponding to the time series\n",
      "Weekly: id W246.\n",
      "ACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\n",
      "t\n",
      "0 0.630056 0.629703 0.625108 0.639236 -0.009534 0.625416 -0.000309\n",
      "1 0.607536 0.643509 0.639846 0.647549 -0.004039 0.639592 0.000254\n",
      "2 0.641731 0.656171 0.652584 0.656696 -0.000526 0.643665 0.008919\n",
      "3 0.628783 0.669636 0.661163 0.666739 0.002897 0.652107 0.009056\n",
      "4 0.816799 0.687287 0.683860 0.677738 0.009549 0.662176 0.021683\n",
      "5 0.817020 0.709211 0.717187 0.689752 0.019459 0.686589 0.030598\n",
      "6 0.766724 0.731732 0.742824 0.702841 0.028891 0.705234 0.037590\n",
      "7 0.770320 0.750834 0.755154 0.717066 0.033768 0.716986 0.038167\n",
      "8 0.794113 0.769671 0.778460 0.732487 0.037184 0.731113 0.047347\n",
      "9 0.874011 0.793373 0.810332 0.749164 0.044209 0.750939 0.059392\n",
      "10 1.000000 0.816386 0.847545 0.767157 0.049229 0.776405 0.071140\n",
      "11 0.979251 0.834532 0.858604 0.786526 0.048006 0.783939 0.074665\n",
      "12 0.933160 0.850010 0.866116 0.807332 0.042678 0.792134 0.073982\n",
      "Table 23: Detailed traces of signals depicted in row 5 of Fig. 5, corresponding to the time series\n",
      "Daily: id D404.\n",
      "ACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\n",
      "t\n",
      "0 0.968704 0.972314 0.971950 0.972589 -0.000275 0.972964 -0.001014\n",
      "1 0.954319 0.972637 0.972131 0.972808 -0.000171 0.972822 -0.000690\n",
      "2 0.954599 0.972972 0.972188 0.973060 -0.000088 0.973798 -0.001610\n",
      "3 0.959959 0.973230 0.972140 0.973341 -0.000112 0.973686 -0.001546\n",
      "4 0.975472 0.973481 0.972125 0.973649 -0.000168 0.974060 -0.001934\n",
      "5 0.970391 0.973715 0.972174 0.973979 -0.000264 0.974800 -0.002626\n",
      "6 0.977728 0.974056 0.972403 0.974328 -0.000272 0.974368 -0.001965\n",
      "7 0.985624 0.974445 0.972428 0.974693 -0.000248 0.973870 -0.001442\n",
      "8 0.979695 0.974823 0.972567 0.975069 -0.000246 0.974870 -0.002303\n",
      "9 0.985345 0.975079 0.973089 0.975455 -0.000376 0.975970 -0.002881\n",
      "10 0.983088 0.975547 0.973881 0.975845 -0.000298 0.975796 -0.001915\n",
      "11 0.983368 0.975991 0.974537 0.976238 -0.000247 0.976757 -0.002220\n",
      "12 0.998312 0.976365 0.974924 0.976628 -0.000263 0.977579 -0.002655\n",
      "13 1.000000 0.976821 0.975291 0.977013 -0.000193 0.977213 -0.001922\n",
      "30' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 29}\n",
      "page_content='Published as a conference paper at ICLR 2020\n",
      "Table 24: Detailed traces of signals depicted in row 6 of Fig. 5, corresponding to the time series\n",
      "Hourly: id H344.\n",
      "ACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\n",
      "t\n",
      "0 0.226804 0.256799 0.277159 0.346977 -0.090179 0.280489 -0.003329\n",
      "1 0.175258 0.228913 0.234605 0.347615 -0.118701 0.241790 -0.007185\n",
      "2 0.164948 0.209208 0.207347 0.348265 -0.139057 0.218575 -0.011228\n",
      "3 0.164948 0.197360 0.193084 0.348928 -0.151568 0.208458 -0.015374\n",
      "4 0.216495 0.190397 0.186586 0.349606 -0.159209 0.205701 -0.019115\n",
      "5 0.195876 0.194204 0.189433 0.350297 -0.156094 0.214399 -0.024966\n",
      "6 0.319588 0.221026 0.216221 0.351004 -0.129978 0.241574 -0.025353\n",
      "7 0.226804 0.279857 0.276414 0.351726 -0.071869 0.293580 -0.017167\n",
      "8 0.371134 0.357292 0.359372 0.352464 0.004828 0.364392 -0.005020\n",
      "9 0.536082 0.438540 0.446126 0.353218 0.085322 0.442703 0.003423\n",
      "10 0.711340 0.511441 0.519928 0.353989 0.157452 0.510142 0.009787\n",
      "11 0.752577 0.571604 0.578186 0.354777 0.216827 0.571596 0.006590\n",
      "12 0.783505 0.617085 0.618778 0.355584 0.261501 0.613425 0.005353\n",
      "13 0.773196 0.651777 0.655123 0.356409 0.295368 0.649259 0.005864\n",
      "14 0.618557 0.670202 0.676814 0.357253 0.312950 0.669555 0.007260\n",
      "15 0.793814 0.679884 0.692592 0.358116 0.321768 0.684208 0.008384\n",
      "16 0.793814 0.672488 0.696440 0.359000 0.313488 0.684764 0.011676\n",
      "17 0.680412 0.648851 0.677696 0.359904 0.288947 0.662714 0.014983\n",
      "18 0.525773 0.602496 0.630922 0.360828 0.241667 0.620368 0.010554\n",
      "19 0.505155 0.537698 0.552296 0.361775 0.175923 0.552599 -0.000304\n",
      "20 0.701031 0.463760 0.466442 0.362743 0.101016 0.477429 -0.010987\n",
      "21 0.484536 0.395795 0.390958 0.363734 0.032061 0.408708 -0.017750\n",
      "22 0.247423 0.337809 0.338500 0.364748 -0.026939 0.354028 -0.015528\n",
      "23 0.371134 0.292452 0.303902 0.365786 -0.073334 0.312588 -0.008686\n",
      "24 0.216495 0.254359 0.258435 0.366848 -0.112489 0.270568 -0.012133\n",
      "25 0.412371 0.227557 0.224291 0.367934 -0.140377 0.237846 -0.013555\n",
      "26 0.237113 0.207962 0.201250 0.369046 -0.161084 0.219420 -0.018169\n",
      "27 0.206186 0.196049 0.189439 0.370183 -0.174133 0.209743 -0.020304\n",
      "28 0.206186 0.189030 0.182843 0.371346 -0.182316 0.207727 -0.024884\n",
      "29 0.237113 0.194524 0.185734 0.372536 -0.178011 0.213194 -0.027460\n",
      "30 0.206186 0.220227 0.215444 0.373753 -0.153526 0.242485 -0.027041\n",
      "31 0.329897 0.279614 0.274624 0.374998 -0.095383 0.292834 -0.018210\n",
      "32 0.371134 0.355078 0.358020 0.376270 -0.021193 0.365332 -0.007312\n",
      "33 0.494845 0.437103 0.445832 0.377572 0.059531 0.441323 0.004510\n",
      "34 0.690722 0.509515 0.520006 0.378903 0.130612 0.512064 0.007942\n",
      "35 0.989691 0.570761 0.579003 0.380263 0.190497 0.569851 0.009152\n",
      "36 1.000000 0.615868 0.623981 0.381654 0.234214 0.617254 0.006728\n",
      "37 0.845361 0.651487 0.656782 0.383076 0.268411 0.650336 0.006446\n",
      "38 0.742268 0.670664 0.678412 0.384528 0.286136 0.673055 0.005357\n",
      "39 0.721649 0.680534 0.691961 0.386013 0.294521 0.684347 0.007614\n",
      "40 0.567010 0.671607 0.692853 0.387530 0.284078 0.683297 0.009555\n",
      "41 0.546392 0.648851 0.672476 0.389079 0.259771 0.660613 0.011863\n",
      "42 0.432990 0.599785 0.621940 0.390662 0.209123 0.615426 0.006514\n",
      "43 0.391753 0.537520 0.544543 0.392279 0.145241 0.549961 -0.005417\n",
      "44 0.443299 0.462772 0.457700 0.393930 0.068842 0.471080 -0.013380\n",
      "45 0.422680 0.397098 0.380324 0.395616 0.001482 0.401229 -0.020905\n",
      "46 0.381443 0.342213 0.325583 0.397337 -0.055124 0.347827 -0.022244\n",
      "47 0.257732 0.297711 0.287130 0.399094 -0.101384 0.304270 -0.017140\n",
      "31' metadata={'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': 30}\n"
     ]
    }
   ],
   "source": [
    "for page in data:\n",
    "    print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cfe2f99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:14:11.160060Z",
     "start_time": "2025-01-23T04:14:11.151672Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Published as a conference paper at ICLR 2020\\ninject a suitable inductive bias in the model to make its internal operations more interpretable, in the\\nsense of extracting some explainable driving factors combining to produce a given forecast?\\n1.1 S UMMARY OF CONTRIBUTIONS\\nDeep Neural Architecture: To the best of our knowledge, this is the ﬁrst work to empirically\\ndemonstrate that pure DL using no time-series speciﬁc components outperforms well-established\\nstatistical approaches on M3, M4 and TOURISM datasets (on M4, by 11% over statistical benchmark,\\nby 7% over the best statistical entry, and by 3% over the M4 competition winner). In our view, this\\nprovides a long-missing proof of concept for the use of pure ML in TS forecasting and strengthens\\nmotivation to continue advancing the research in this area.\\nInterpretable DL for Time Series: In addition to accuracy beneﬁts, we also show that it is fea-\\nsible to design an architecture with interpretable outputs that can be used by practitioners in very\\nmuch the same way as traditional decomposition techniques such as the “seasonality-trend-level”\\napproach (Cleveland et al., 1990).\\n2 P ROBLEM STATEMENT\\nWe consider the univariate point forecasting problem in discrete time. Given a length- Hforecast\\nhorizon a length- Tobserved series history [y1,..., yT]∈RT, the task is to predict the vector of\\nfuture values y∈RH= [yT+1,yT+2,..., yT+H]. For simplicity, we will later consider a lookback\\nwindow of length t≤Tending with the last observed value yTto serve as model input, and denoted\\nx∈Rt= [yT−t+1,..., yT]. We denoteˆythe forecast of y. The following metrics are commonly\\nused to evaluate forecasting performance (Hyndman & Koehler, 2006; Makridakis & Hibon, 2000;\\nMakridakis et al., 2018b; Athanasopoulos et al., 2011):\\nsMAPE =200\\nHH\\n∑\\ni=1|yT+i−ˆyT+i|\\n|yT+i|+|ˆyT+i|, MAPE =100\\nHH\\n∑\\ni=1|yT+i−ˆyT+i|\\n|yT+i|,\\nMASE =1\\nHH\\n∑\\ni=1|yT+i−ˆyT+i|\\n1\\nT+H−m∑T+H\\nj=m+1|yj−yj−m|, OWA=1\\n2[sMAPE\\nsMAPE Naïve2+MASE\\nMASE Naïve2]\\n.\\nHere mis the periodicity of the data ( e.g., 12 for monthly series). MAPE (Mean Absolute Percentage\\nError), sMAPE (symmetric MAPE ) and MASE (Mean Absolute Scaled Error) are standard scale-free\\nmetrics in the practice of forecasting (Hyndman & Koehler, 2006; Makridakis & Hibon, 2000):\\nwhereas sMAPE scales the error by the average between the forecast and ground truth, the MASE\\nscales by the average error of the naïve predictor that simply copies the observation measured m\\nperiods in the past, thereby accounting for seasonality. OWA (overall weighted average) is a M4-\\nspeciﬁc metric used to rank competition entries (M4 Team, 2018b), where sMAPE and MASE metrics\\nare normalized such that a seasonally-adjusted naïve forecast obtains OWA=1.0.\\n3 N-BEATS\\nOur architecture design methodology relies on a few key principles. First, the base architecture\\nshould be simple and generic, yet expressive (deep). Second, the architecture should not rely on time-\\nseries-speciﬁc feature engineering or input scaling. These prerequisites let us explore the potential\\nof pure DL architecture in TS forecasting. Finally, as a prerequisite to explore interpretability, the\\narchitecture should be extendable towards making its outputs human interpretable. We now discuss\\nhow those principles converge to the proposed architecture.\\n3.1 B ASIC BLOCK\\nThe proposed basic building block has a fork architecture and is depicted in Fig. 1 (left). We focus on\\ndescribing the operation of ℓ-th block in this section in detail (note that the block index ℓis dropped\\nin Fig. 1 for brevity). The ℓ-th block accepts its respective input xℓand outputs two vectors, ˆxℓand\\nˆyℓ. For the very ﬁrst block in the model, its respective xℓis the overall model input — a history\\nlookback window of certain length ending with the last measured observation. We set the length of\\n2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_page = data[1] \n",
    "\n",
    "first_page.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e8903",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T19:40:30.567492Z",
     "start_time": "2025-01-22T19:40:30.560372Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc053f03",
   "metadata": {},
   "source": [
    "# Cleaning The Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "535d255f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:14:11.502894Z",
     "start_time": "2025-01-23T04:14:11.497275Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain_core.documents import Document\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a3e7f46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:14:11.989641Z",
     "start_time": "2025-01-23T04:14:11.980647Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by:\n",
    "    - Removing special characters that are not relevant to English text.\n",
    "    - Replacing consecutive periods, commas, exclamation marks, or question marks with a single one.\n",
    "    - Collapsing multiple spaces into a single space.\n",
    "    - Removing leading and trailing whitespace.\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"[^\\w\\s\\.\\,\\?\\!\\-']\", \"\", text)\n",
    "    \n",
    "    text = re.sub(r\"(\\.{2,})\", \".\", text)  \n",
    "    text = re.sub(r\"(\\,{2,})\", \",\", text)  \n",
    "    text = re.sub(r\"(\\!{2,})\", \"!\", text)  \n",
    "    text = re.sub(r\"(\\?{2,})\", \"?\", text)  \n",
    "    \n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fd4aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "796897a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:14:12.916914Z",
     "start_time": "2025-01-23T04:14:12.913548Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_docs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7c44bba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:14:13.200667Z",
     "start_time": "2025-01-23T04:14:13.176285Z"
    }
   },
   "outputs": [],
   "source": [
    "for page in data:\n",
    "    cleaned_page =Document(page_content=clean_text(page.page_content),metadata=page.metadata)\n",
    "    cleaned_docs.append(cleaned_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b65bc15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T19:50:11.750494Z",
     "start_time": "2025-01-22T19:50:11.746112Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbee8577",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:14:13.898629Z",
     "start_time": "2025-01-23T04:14:13.893168Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ans = len(reduce(lambda x, y: x + y.page_content, data, \"\"))\n",
    "# ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67a1235d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:14:14.346072Z",
     "start_time": "2025-01-23T04:14:14.337493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Character Before Cleaning 91599\n",
      "Total Character After Cleaning 89917\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Character Before Cleaning {len(reduce(lambda x, y: x + y.page_content, data, ''))}\")\n",
    "print(f\"Total Character After Cleaning {len(reduce(lambda x, y: x + y.page_content, cleaned_docs, ''))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17914be4",
   "metadata": {},
   "source": [
    "* There are most advanced type of cleanings also we can perform using pdfplumber, pdfminer,pymupdf and etc. Due to time constraints I'm sticking to basic ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887476ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e58688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f646be1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T19:59:36.752160Z",
     "start_time": "2025-01-22T19:59:36.729166Z"
    }
   },
   "source": [
    "# Splitting The Document Into Chunks\n",
    "\n",
    "* Let's go with SemanticChunker instead of CharacterTextSplitter or Recursive text splitters and etc\n",
    "* SemanticChunker Requires Embedding models to work - it works by analyzing the relevance of a particular sentence with the following sentences, until the provided threshold exceeds, it keeps on appending those sentences into a single chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7087457",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:14:18.949538Z",
     "start_time": "2025-01-23T04:14:18.933378Z"
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35847776",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:14:19.674184Z",
     "start_time": "2025-01-23T04:14:19.660684Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"devenv.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1488e70e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:14:20.075151Z",
     "start_time": "2025-01-23T04:14:20.068775Z"
    }
   },
   "outputs": [],
   "source": [
    "def semantic_text_splitter(text_splitter,documents):\n",
    "    \"\"\"\n",
    "    Splits the input text into semantically meaningful chunks using LangChain's SemanticChunker.\n",
    "    \"\"\"\n",
    "    semantic_chunks = text_splitter.split_documents(documents)\n",
    "    return semantic_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07cbb54a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:14:23.346462Z",
     "start_time": "2025-01-23T04:14:21.301098Z"
    }
   },
   "outputs": [],
   "source": [
    "text_splitter = SemanticChunker(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0e91776",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:15:08.788386Z",
     "start_time": "2025-01-23T04:14:23.535079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.98 s\n",
      "Wall time: 45.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "semantic_chunks = semantic_text_splitter(text_splitter,cleaned_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c3c5643",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:15:08.803645Z",
     "start_time": "2025-01-23T04:15:08.795421Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(semantic_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4dacd1b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:15:08.812991Z",
     "start_time": "2025-01-23T04:15:08.805859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published as a conference paper at ICLR 2020 N-BEATS N EURAL BASIS EXPANSION ANALYSIS FOR INTERPRETABLE TIME SERIES FORECASTING Boris N. Oreshkin Element AI boris.oreshkingmail.comDmitri Carpov Element AI dmitri.carpovelementai.com Nicolas Chapados Element AI chapadoselementai.comYoshua Bengio Mila yoshua.bengiomila.quebec ABSTRACT We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers.\n",
      "\n",
      "\n",
      "The ar- chitecture has a number of desirable properties, being interpretable, applicable without modiﬁcation to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two conﬁgurations of N-BEATS for all the datasets, improving forecast accuracy by 11 over a statistical benchmark and by 3 over last years winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The ﬁrst conﬁguration of our model does not employ any time-series-speciﬁc components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufﬁcient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy. 1 I NTRODUCTION Time series TS forecasting is an important business problem and a fruitful application area for machine learning ML. It underlies most aspects of modern business, including such critical areas as inventory control and customer management, as well as business planning going from production and distribution to ﬁnance and marketing. As such, it has a considerable ﬁnancial impact, often ranging in the millions of dollars for every point of forecasting accuracy gained Jain, 2017 Kahn, 2003. And yet, unlike areas such as computer vision or natural language processing where deep learning DL techniques are now well entrenched, there still exists evidence that ML and DL struggle to outperform classical statistical TS forecasting approaches Makridakis et al., 2018ab. For instance, the rankings of the six pure ML methods submitted to M4 competition were 23, 37, 38, 48, 54, and 57 out of a total of 60 entries, and most of the best-ranking methods were ensembles of classical statistical techniques Makridakis et al., 2018b. On the other hand, the M4 competition winner Smyl, 2020, was based on a hybrid between neural residualattention dilated LSTM stack with a classical Holt-Winters statistical model Holt, 1957 2004 Winters, 1960 with learnable parameters. Since Smyls approach heavily depends on this Holt-Winters component, Makridakis et al. 2018b further argue that hybrid approaches and combinations of method are the way forward for improving the forecasting accuracy and making forecasting more valuable. In this work we aspire to challenge this conclusion by exploring the potential of pure DL architectures in the context of the TS forecasting. Moreover, in the context of interpretable DL architecture design, we are interested in answering the following question can we 1arXiv1905.10437v4 cs.LG 20 Feb 2020\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2020 inject a suitable inductive bias in the model to make its internal operations more interpretable, in the sense of extracting some explainable driving factors combining to produce a given forecast? 1.1 S UMMARY OF CONTRIBUTIONS Deep Neural Architecture To the best of our knowledge, this is the ﬁrst work to empirically demonstrate that pure DL using no time-series speciﬁc components outperforms well-established statistical approaches on M3, M4 and TOURISM datasets on M4, by 11 over statistical benchmark, by 7 over the best statistical entry, and by 3 over the M4 competition winner. In our view, this provides a long-missing proof of concept for the use of pure ML in TS forecasting and strengthens motivation to continue advancing the research in this area. Interpretable DL for Time Series In addition to accuracy beneﬁts, we also show that it is fea- sible to design an architecture with interpretable outputs that can be used by practitioners in very much the same way as traditional decomposition techniques such as the seasonality-trend-level approach Cleveland et al., 1990. 2 P ROBLEM STATEMENT We consider the univariate point forecasting problem in discrete time. Given a length- Hforecast horizon a length- Tobserved series history y1,., yTRT, the task is to predict the vector of future values yRH yT1,yT2,., yTH. For simplicity, we will later consider a lookback window of length tTending with the last observed value yTto serve as model input, and denoted xRt yTt1,., yT. We denoteˆythe forecast of y. The following metrics are commonly used to evaluate forecasting performance Hyndman Koehler, 2006 Makridakis Hibon, 2000 Makridakis et al., 2018b Athanasopoulos et al., 2011 sMAPE 200 HH i1yTiˆyTi yTiˆyTi, MAPE 100 HH i1yTiˆyTi yTi, MASE 1 HH i1yTiˆyTi 1 THmTH jm1yjyjm, OWA1 2sMAPE sMAPE Naïve2MASE MASE Naïve2 . Here mis the periodicity of the data e.g., 12 for monthly series. MAPE Mean Absolute Percentage Error, sMAPE symmetric MAPE and MASE Mean Absolute Scaled Error are standard scale-free metrics in the practice of forecasting Hyndman Koehler, 2006 Makridakis Hibon, 2000 whereas sMAPE scales the error by the average between the forecast and ground truth, the MASE scales by the average error of the naïve predictor that simply copies the observation measured m periods in the past, thereby accounting for seasonality. OWA overall weighted average is a M4- speciﬁc metric used to rank competition entries M4 Team, 2018b, where sMAPE and MASE metrics are normalized such that a seasonally-adjusted naïve forecast obtains OWA1.0. 3 N-BEATS Our architecture design methodology relies on a few key principles.\n",
      "\n",
      "\n",
      "First, the base architecture should be simple and generic, yet expressive deep. Second, the architecture should not rely on time- series-speciﬁc feature engineering or input scaling. These prerequisites let us explore the potential of pure DL architecture in TS forecasting. Finally, as a prerequisite to explore interpretability, the architecture should be extendable towards making its outputs human interpretable.\n",
      "\n",
      "\n",
      "We now discuss how those principles converge to the proposed architecture. 3.1 B ASIC BLOCK The proposed basic building block has a fork architecture and is depicted in Fig. 1 left. We focus on describing the operation of ℓ-th block in this section in detail note that the block index ℓis dropped in Fig. 1 for brevity. The ℓ-th block accepts its respective input xℓand outputs two vectors, ˆxℓand ˆyℓ. For the very ﬁrst block in the model, its respective xℓis the overall model input a history lookback window of certain length ending with the last measured observation. We set the length of 2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in semantic_chunks[:5]:\n",
    "    print(chunk.page_content,end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ce92c7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:27:44.150252Z",
     "start_time": "2025-01-23T04:27:44.143282Z"
    }
   },
   "outputs": [],
   "source": [
    "def setup_redis_vector_store(split_docs, redis_url=\"redis://localhost:6379\", index_name=\"langchain-index\"):\n",
    "    \"\"\"\n",
    "    Sets up the Redis vector store using LangChain's Redis integration.\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vector_store = Redis.from_documents(\n",
    "        split_docs,\n",
    "        embedding=embeddings,\n",
    "        redis_url=redis_url,\n",
    "        index_name=index_name,\n",
    "    )\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9bda324b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:38:17.022540Z",
     "start_time": "2025-01-23T04:38:17.016310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Redis at: redis://localhost:6379\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "print(f\"Connecting to Redis at: {REDIS_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dd472745",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:38:17.249818Z",
     "start_time": "2025-01-23T04:38:17.236495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import redis\n",
    "\n",
    "redis_client = redis.from_url(REDIS_URL)\n",
    "redis_client.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dfc792e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:42:16.830588Z",
     "start_time": "2025-01-23T04:42:12.711544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.5 s\n",
      "Wall time: 4.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "redis_store = setup_redis_vector_store(semantic_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2ae2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ee46698",
   "metadata": {},
   "source": [
    "## Deleting Index From the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ac8edcd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:42:04.097861Z",
     "start_time": "2025-01-23T04:42:04.090701Z"
    }
   },
   "outputs": [],
   "source": [
    "redis_client = redis.from_url(REDIS_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "12788c53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T09:16:11.970657Z",
     "start_time": "2025-01-23T09:16:11.961256Z"
    }
   },
   "outputs": [],
   "source": [
    "index_name = \"doc_index\"\n",
    "index_keys = redis_client.keys(f\"{index_name}:*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "decb8a28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T09:16:12.394450Z",
     "start_time": "2025-01-23T09:16:12.384211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'doc_index:8ff42cf01dde4fe3aa0ee0f3d35d07af',\n",
       " b'doc_index:c5f970d0b01b4554bae36f83a9e84d90']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_keys[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b2699afa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:43:11.527675Z",
     "start_time": "2025-01-23T04:43:11.522675Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can uncomment this when we want to delete\n",
    "\n",
    "# if index_keys:\n",
    "#     redis_client.delete(*index_keys)\n",
    "#     print(f\"Index '{index_name}' and all associated keys deleted successfully.\")\n",
    "# else:\n",
    "#     print(f\"No keys found for index '{index_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e37738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a17f9ae2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T09:50:57.523848Z",
     "start_time": "2025-01-23T09:50:57.516149Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_from_vector_store(query, vector_store, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieves the most similar documents from the Redis vector store for a given query.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the top_k most similar documents\n",
    "    results = vector_store.similarity_search_with_score(query, k=top_k)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d0490d72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T09:50:59.935475Z",
     "start_time": "2025-01-23T09:50:59.115348Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "query = \"What is the time series approach that is proposed ?\"\n",
    "top_k_results = retrieve_from_vector_store(query, redis_store, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ee0fdc59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T09:50:59.950439Z",
     "start_time": "2025-01-23T09:50:59.941577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata : {'id': 'doc:langchain-index:f056571c056f49889292947975e39d00', 'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': '0'} \n",
      " Similarity Score : 0.1718\n",
      "\n",
      "\n",
      "Metadata : {'id': 'doc:langchain-index:9552189be45e489e9d14bc2ead15a646', 'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': '1'} \n",
      " Similarity Score : 0.1725\n",
      "\n",
      "\n",
      "Metadata : {'id': 'doc:langchain-index:f137d5bc06cf4feeb5b6eb001bb5978c', 'source': 'C:\\\\Users\\\\KeesaramRupesh\\\\Downloads\\\\NBEATS paper.pdf', 'page': '3'} \n",
      " Similarity Score : 0.1733\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "for result in top_k_results:\n",
    "    print(f\"Metadata : {result[0].metadata} \\n Similarity Score : {result[-1]}\",end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b1561eab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:56:50.127916Z",
     "start_time": "2025-01-23T04:56:50.120402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published as a conference paper at ICLR 2020 N-BEATS N EURAL BASIS EXPANSION ANALYSIS FOR INTERPRETABLE TIME SERIES FORECASTING Boris N. Oreshkin Element AI boris.oreshkingmail.comDmitri Carpov Element AI dmitri.carpovelementai.com Nicolas Chapados Element AI chapadoselementai.comYoshua Bengio Mila yoshua.bengiomila.quebec ABSTRACT We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers.\n",
      "\n",
      "\n",
      "Published as a conference paper at ICLR 2020 inject a suitable inductive bias in the model to make its internal operations more interpretable, in the sense of extracting some explainable driving factors combining to produce a given forecast? 1.1 S UMMARY OF CONTRIBUTIONS Deep Neural Architecture To the best of our knowledge, this is the ﬁrst work to empirically demonstrate that pure DL using no time-series speciﬁc components outperforms well-established statistical approaches on M3, M4 and TOURISM datasets on M4, by 11 over statistical benchmark, by 7 over the best statistical entry, and by 3 over the M4 competition winner. In our view, this provides a long-missing proof of concept for the use of pure ML in TS forecasting and strengthens motivation to continue advancing the research in this area. Interpretable DL for Time Series In addition to accuracy beneﬁts, we also show that it is fea- sible to design an architecture with interpretable outputs that can be used by practitioners in very much the same way as traditional decomposition techniques such as the seasonality-trend-level approach Cleveland et al., 1990. 2 P ROBLEM STATEMENT We consider the univariate point forecasting problem in discrete time. Given a length- Hforecast horizon a length- Tobserved series history y1,., yTRT, the task is to predict the vector of future values yRH yT1,yT2,., yTH. For simplicity, we will later consider a lookback window of length tTending with the last observed value yTto serve as model input, and denoted xRt yTt1,., yT. We denoteˆythe forecast of y. The following metrics are commonly used to evaluate forecasting performance Hyndman Koehler, 2006 Makridakis Hibon, 2000 Makridakis et al., 2018b Athanasopoulos et al., 2011 sMAPE 200 HH i1yTiˆyTi yTiˆyTi, MAPE 100 HH i1yTiˆyTi yTi, MASE 1 HH i1yTiˆyTi 1 THmTH jm1yjyjm, OWA1 2sMAPE sMAPE Naïve2MASE MASE Naïve2 . Here mis the periodicity of the data e.g., 12 for monthly series. MAPE Mean Absolute Percentage Error, sMAPE symmetric MAPE and MASE Mean Absolute Scaled Error are standard scale-free metrics in the practice of forecasting Hyndman Koehler, 2006 Makridakis Hibon, 2000 whereas sMAPE scales the error by the average between the forecast and ground truth, the MASE scales by the average error of the naïve predictor that simply copies the observation measured m periods in the past, thereby accounting for seasonality. OWA overall weighted average is a M4- speciﬁc metric used to rank competition entries M4 Team, 2018b, where sMAPE and MASE metrics are normalized such that a seasonally-adjusted naïve forecast obtains OWA1.0. 3 N-BEATS Our architecture design methodology relies on a few key principles.\n",
      "\n",
      "\n",
      "1 and by adding structure to basis layers at stack level. Forecasting practitioners often use the decomposition of time series into trend and seasonality, such as those performed by the STLCleveland et al., 1990 and X13-ARIMA U.S. Census Bureau, 2013. We propose to design the trend and seasonality decomposition into the model to make the stack outputs more easily interpretable. Note 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "for result in top_k_results:\n",
    "    print(result[0].page_content,end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f0a560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
